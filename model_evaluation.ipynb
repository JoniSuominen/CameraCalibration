{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colour Correction using Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Enable automatic reloading of modules before executing code to ensure latest changes are used.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Importing necessary libraries and functions.\n",
    "# 'colour' is used for color science computations.\n",
    "# 'data' contains functions to load various datasets and color matching functions.\n",
    "import colour\n",
    "from data import (load_dataset_sfu, load_dataset_csv, load_cmfs, \n",
    "                  load_camera, load_insitu, msds_to_rgb, msds_to_xyz, load_dataset_skin)\n",
    "import numpy as np\n",
    "\n",
    "# Setting a seed for random number generation to ensure reproducibility.\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Defining file paths for the datasets. These files should contain the data needed for analysis.\n",
    "SFU_FILE_PATH = 'data/reflect_db.reflect'  # SFU material reflectance database\n",
    "CAVE_FOSTER2004_PATH = 'data/cave_foster2002.csv'  # CAVE dataset for color constancy\n",
    "INSITU_PATH = \"data/insitu_dataset.csv\"  # In-situ measurements dataset\n",
    "CAVE_PATH = 'data/cave.csv'  # Another CAVE dataset file\n",
    "FOSTER_50_PATH = 'data/foster50.csv'  # Foster dataset with 50 images for color analysis\n",
    "CAMERA = 'sigma'  # Specifying the camera model used in the datasets or for simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_foster2004_dataset = load_dataset_csv(CAVE_FOSTER2004_PATH)\n",
    "foster_50_dataset = load_dataset_csv(FOSTER_50_PATH)\n",
    "insitu = load_insitu(INSITU_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = cave_foster2004_dataset\n",
    "TEST = foster_50_dataset\n",
    "VALIDATION = cave_foster2004_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Observer Responses\n",
    "We can easily change the order of test and train sets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_chromaticity_diagram\n",
    "from colour.characterisation import training_data_sds_to_XYZ\n",
    "\n",
    "cmfs, illuminant = load_cmfs()\n",
    "response_trainset_xyz = msds_to_xyz(TRAIN, cmfs, illuminant)\n",
    "response_testset_xyz = msds_to_xyz(TEST, cmfs, illuminant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Camera Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colour.characterisation import normalise_illuminant, training_data_sds_to_RGB\n",
    "import numpy as np\n",
    "MSDS_TRAIN = load_camera(CAMERA)\n",
    "\n",
    "response_trainset_camera = msds_to_rgb(TRAIN,MSDS_TRAIN, illuminant)\n",
    "response_testset_camera = msds_to_rgb(TEST,MSDS_TRAIN, illuminant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Generalized Additive Model with P-splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nikon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GAMOptimizer\n",
    "from evaluate import pred\n",
    "def test_gam_optimizer(splines_lams_dict, order_value=3):\n",
    "    \"\"\"\n",
    "    Test GAMOptimizer models with different numbers of splines and corresponding lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    - splines_lams_dict: Dictionary where keys are spline numbers and values are the corresponding lambda values for regularization.\n",
    "    - order_value: The order of the spline. Default is 3.\n",
    "    \"\"\"\n",
    "    for n_splines, lams_value in splines_lams_dict.items():\n",
    "        print(f\"Testing GAMOptimizer with {n_splines} splines and lambda {lams_value}\")\n",
    "        gam = GAMOptimizer(lams=lams_value, order=order_value, n_splines=n_splines)\n",
    "        gam.fit(response_trainset_camera, response_trainset_xyz)\n",
    "        pred(gam, response_testset_camera, response_testset_xyz, f\"DeltaE Foster+CAVE with {n_splines} splines and lambda {lams_value}\")\n",
    "\n",
    "# Example usage: Create a dictionary mapping spline numbers to their corresponding lambda values and pass it to the function.\n",
    "splines_lams_dict = {\n",
    "    5: 1e-6,\n",
    "    10: 0.0001,\n",
    "    20: 0.01\n",
    "}\n",
    "if CAMERA == 'nikon':\n",
    "    test_gam_optimizer(splines_lams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gam_optimizer(splines_lams_dict, order_value=3):\n",
    "    \"\"\"\n",
    "    Test GAMOptimizer models with different numbers of splines and corresponding lambda values.\n",
    "\n",
    "    Parameters:\n",
    "    - splines_lams_dict: Dictionary where keys are spline numbers and values are the corresponding lambda values for regularization.\n",
    "    - order_value: The order of the spline. Default is 3.\n",
    "    \"\"\"\n",
    "    for n_splines, lams_value in splines_lams_dict.items():\n",
    "        print(f\"Testing GAMOptimizer with {n_splines} splines and lambda {lams_value}\")\n",
    "        gam = GAMOptimizer(lams=lams_value, order=order_value, n_splines=n_splines)\n",
    "        gam.fit(response_trainset_camera, response_trainset_xyz)\n",
    "        pred(gam, response_testset_camera, response_testset_xyz, f\"{n_splines} splines and lambda {lams_value}\")\n",
    "\n",
    "# Example usage: Create a dictionary mapping spline numbers to their corresponding lambda values and pass it to the function.\n",
    "splines_lams_dict = {\n",
    "    5: 1e-9,\n",
    "    10: 0.0001,\n",
    "    20: 0.0001\n",
    "}\n",
    "if CAMERA == 'sigma':\n",
    "    test_gam_optimizer(splines_lams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 30.058632578168595, Validation Loss: 29.240988731384277\n",
      "Epoch 2, Train Loss: 26.26083483014788, Validation Loss: 26.54704189300537\n",
      "Epoch 3, Train Loss: 24.2638156073434, Validation Loss: 24.954862117767334\n",
      "Epoch 4, Train Loss: 22.186376776014054, Validation Loss: 24.29143524169922\n",
      "Epoch 5, Train Loss: 21.836743354797363, Validation Loss: 24.049461364746094\n",
      "Epoch 6, Train Loss: 21.684913771493093, Validation Loss: 24.14634418487549\n",
      "Epoch 7, Train Loss: 21.680494035993302, Validation Loss: 23.839200019836426\n",
      "Epoch 8, Train Loss: 21.302258150918142, Validation Loss: 25.334771633148193\n",
      "Epoch 9, Train Loss: 21.611083848135813, Validation Loss: 23.934657096862793\n",
      "Epoch 10, Train Loss: 21.114169597625732, Validation Loss: 23.711734294891357\n",
      "Epoch 11, Train Loss: 21.28043236051287, Validation Loss: 23.77201747894287\n",
      "Epoch 12, Train Loss: 21.565985815865652, Validation Loss: 23.417031288146973\n",
      "Epoch 13, Train Loss: 21.031372138432094, Validation Loss: 24.191319942474365\n",
      "Epoch 14, Train Loss: 21.247858251844132, Validation Loss: 23.264657497406006\n",
      "Epoch 15, Train Loss: 21.09965440205165, Validation Loss: 23.54721164703369\n",
      "Epoch 16, Train Loss: 21.60512351989746, Validation Loss: 26.69560670852661\n",
      "Epoch 17, Train Loss: 21.591071810041154, Validation Loss: 23.51240825653076\n",
      "Epoch 18, Train Loss: 20.992987087794713, Validation Loss: 24.467015266418457\n",
      "Epoch 19, Train Loss: 21.158474240984237, Validation Loss: 23.502490997314453\n",
      "Epoch 20, Train Loss: 20.92903184890747, Validation Loss: 23.197973251342773\n",
      "Epoch 21, Train Loss: 20.88386787687029, Validation Loss: 23.19780731201172\n",
      "Epoch 22, Train Loss: 20.936876841953822, Validation Loss: 22.963956832885742\n",
      "Epoch 23, Train Loss: 21.034871305738175, Validation Loss: 25.40924644470215\n",
      "Epoch 24, Train Loss: 21.907201017652238, Validation Loss: 24.017999172210693\n",
      "Epoch 25, Train Loss: 21.08324589048113, Validation Loss: 22.982199668884277\n",
      "Epoch 26, Train Loss: 21.12547745023455, Validation Loss: 25.77617645263672\n",
      "Epoch 27, Train Loss: 21.895133904048375, Validation Loss: 23.028135299682617\n",
      "Epoch 28, Train Loss: 21.5726478440421, Validation Loss: 23.22141981124878\n",
      "Epoch 29, Train Loss: 20.97229548863002, Validation Loss: 22.910693645477295\n",
      "Epoch 30, Train Loss: 20.84051833833967, Validation Loss: 23.594579696655273\n",
      "Epoch 31, Train Loss: 21.061202798570907, Validation Loss: 23.04183530807495\n",
      "Epoch 32, Train Loss: 20.756893021719797, Validation Loss: 23.009129524230957\n",
      "Epoch 33, Train Loss: 20.885639122554235, Validation Loss: 23.316214561462402\n",
      "Epoch 34, Train Loss: 20.816945825304305, Validation Loss: 23.23681402206421\n",
      "Epoch 35, Train Loss: 20.584667682647705, Validation Loss: 23.10106372833252\n",
      "Epoch 36, Train Loss: 20.88566337312971, Validation Loss: 23.806435108184814\n",
      "Epoch 37, Train Loss: 20.901696818215505, Validation Loss: 23.741732597351074\n",
      "Epoch 38, Train Loss: 20.58211878367833, Validation Loss: 22.928672313690186\n",
      "Epoch 39, Train Loss: 20.44771773474557, Validation Loss: 22.882097244262695\n",
      "Epoch 40, Train Loss: 20.79090929031372, Validation Loss: 22.80823278427124\n",
      "Epoch 41, Train Loss: 21.617880753108434, Validation Loss: 23.076260089874268\n",
      "Epoch 42, Train Loss: 21.042558942522323, Validation Loss: 22.992043495178223\n",
      "Epoch 43, Train Loss: 20.604078905923025, Validation Loss: 22.6781325340271\n",
      "Epoch 44, Train Loss: 20.560026713779994, Validation Loss: 22.702160358428955\n",
      "Epoch 45, Train Loss: 21.03988061632429, Validation Loss: 23.600682258605957\n",
      "Epoch 46, Train Loss: 20.733374459402903, Validation Loss: 22.85693073272705\n",
      "Epoch 47, Train Loss: 20.429031985146658, Validation Loss: 22.797776699066162\n",
      "Epoch 48, Train Loss: 20.515254838126047, Validation Loss: 22.57048273086548\n",
      "Epoch 49, Train Loss: 20.399127892085485, Validation Loss: 22.769710540771484\n",
      "Epoch 50, Train Loss: 20.764385087149485, Validation Loss: 22.941383361816406\n",
      "Epoch 51, Train Loss: 20.283266884940012, Validation Loss: 22.652417182922363\n",
      "Epoch 52, Train Loss: 20.16436951501029, Validation Loss: 23.838225841522217\n",
      "Epoch 53, Train Loss: 20.650686740875244, Validation Loss: 22.984149932861328\n",
      "Epoch 54, Train Loss: 20.494722366333008, Validation Loss: 23.632038116455078\n",
      "Epoch 55, Train Loss: 20.304065227508545, Validation Loss: 22.63588047027588\n",
      "Epoch 56, Train Loss: 20.64363922391619, Validation Loss: 22.78065824508667\n",
      "Epoch 57, Train Loss: 20.61436585017613, Validation Loss: 23.99527645111084\n",
      "Epoch 58, Train Loss: 20.762638296399796, Validation Loss: 22.288973808288574\n",
      "Epoch 59, Train Loss: 20.47757169178554, Validation Loss: 23.170984745025635\n",
      "Epoch 60, Train Loss: 20.535545417240687, Validation Loss: 22.72377872467041\n",
      "Epoch 61, Train Loss: 20.373688084738596, Validation Loss: 22.604708194732666\n",
      "Epoch 62, Train Loss: 20.425150871276855, Validation Loss: 22.963120937347412\n",
      "Epoch 63, Train Loss: 21.081816264561244, Validation Loss: 23.22022533416748\n",
      "Epoch 64, Train Loss: 20.583327225276403, Validation Loss: 22.798102378845215\n",
      "Epoch 65, Train Loss: 20.367674827575684, Validation Loss: 22.258121013641357\n",
      "Epoch 66, Train Loss: 20.08120346069336, Validation Loss: 22.14728546142578\n",
      "Epoch 67, Train Loss: 20.0710540499006, Validation Loss: 22.450687885284424\n",
      "Epoch 68, Train Loss: 20.237311703818186, Validation Loss: 22.33415460586548\n",
      "Epoch 69, Train Loss: 20.055228028978622, Validation Loss: 22.36201047897339\n",
      "Epoch 70, Train Loss: 20.345741067613876, Validation Loss: 22.88911771774292\n",
      "Epoch 71, Train Loss: 21.3191009249006, Validation Loss: 22.484532356262207\n",
      "Epoch 72, Train Loss: 20.271269457680837, Validation Loss: 23.786832809448242\n",
      "Epoch 73, Train Loss: 20.225600787571498, Validation Loss: 22.387457847595215\n",
      "Epoch 74, Train Loss: 20.33263553891863, Validation Loss: 23.792945384979248\n",
      "Epoch 75, Train Loss: 20.21248980930873, Validation Loss: 22.465270042419434\n",
      "Epoch 76, Train Loss: 20.31443657193865, Validation Loss: 22.360918521881104\n",
      "Epoch 77, Train Loss: 19.951274394989014, Validation Loss: 22.3701229095459\n",
      "Epoch 78, Train Loss: 20.099663530077255, Validation Loss: 22.178586959838867\n",
      "Epoch 79, Train Loss: 20.07636594772339, Validation Loss: 23.152878761291504\n",
      "Epoch 80, Train Loss: 20.576748371124268, Validation Loss: 22.494056701660156\n",
      "Epoch 81, Train Loss: 20.21448312486921, Validation Loss: 22.253730297088623\n",
      "Epoch 82, Train Loss: 20.100341864994594, Validation Loss: 21.98006820678711\n",
      "Epoch 83, Train Loss: 19.91865927832467, Validation Loss: 22.2646164894104\n",
      "Epoch 84, Train Loss: 20.016808373587473, Validation Loss: 22.425739765167236\n",
      "Epoch 85, Train Loss: 19.905019760131836, Validation Loss: 22.006296634674072\n",
      "Epoch 86, Train Loss: 20.278692381722585, Validation Loss: 22.343242645263672\n",
      "Epoch 87, Train Loss: 20.296524456569127, Validation Loss: 22.47091293334961\n",
      "Epoch 88, Train Loss: 20.040759018489293, Validation Loss: 22.046008586883545\n",
      "Epoch 89, Train Loss: 19.872392177581787, Validation Loss: 23.054253578186035\n",
      "Epoch 90, Train Loss: 20.201349190303258, Validation Loss: 23.073416233062744\n",
      "Epoch 91, Train Loss: 20.514873845236643, Validation Loss: 24.00623655319214\n",
      "Epoch 92, Train Loss: 20.605920995984757, Validation Loss: 22.20581579208374\n",
      "Epoch 93, Train Loss: 20.072794573647634, Validation Loss: 22.03097438812256\n",
      "Epoch 94, Train Loss: 19.917214597974503, Validation Loss: 22.300844192504883\n",
      "Epoch 95, Train Loss: 20.007703917367117, Validation Loss: 22.347458839416504\n",
      "Epoch 96, Train Loss: 20.195502962384904, Validation Loss: 22.080857276916504\n",
      "Epoch 97, Train Loss: 19.777348858969553, Validation Loss: 22.3594388961792\n",
      "Epoch 98, Train Loss: 20.250902720860072, Validation Loss: 23.157387256622314\n",
      "Epoch 99, Train Loss: 20.099499361855642, Validation Loss: 21.915812969207764\n",
      "Epoch 100, Train Loss: 19.90990945271083, Validation Loss: 21.982714653015137\n",
      "Epoch 101, Train Loss: 19.873334067208425, Validation Loss: 22.264519691467285\n",
      "Epoch 102, Train Loss: 19.853072302682058, Validation Loss: 23.025856018066406\n",
      "Epoch 103, Train Loss: 20.082350390298025, Validation Loss: 21.853203296661377\n",
      "Epoch 104, Train Loss: 19.99031959261213, Validation Loss: 22.35997200012207\n",
      "Epoch 105, Train Loss: 20.14354964665004, Validation Loss: 22.315531253814697\n",
      "Epoch 106, Train Loss: 20.19368634905134, Validation Loss: 22.208182334899902\n",
      "Epoch 107, Train Loss: 20.369609151567733, Validation Loss: 21.986703872680664\n",
      "Epoch 108, Train Loss: 20.22665398461478, Validation Loss: 23.015809059143066\n",
      "Epoch 109, Train Loss: 20.554468972342356, Validation Loss: 22.707682609558105\n",
      "Epoch 110, Train Loss: 20.035080364772252, Validation Loss: 22.020857334136963\n",
      "Epoch 111, Train Loss: 20.375661849975586, Validation Loss: 22.13116693496704\n",
      "Epoch 112, Train Loss: 19.949195861816406, Validation Loss: 22.02643918991089\n",
      "Epoch 113, Train Loss: 19.779566492353165, Validation Loss: 22.082812309265137\n",
      "Epoch 114, Train Loss: 19.7464143208095, Validation Loss: 21.971724033355713\n",
      "Epoch 115, Train Loss: 19.623686722346715, Validation Loss: 22.860251426696777\n",
      "Epoch 116, Train Loss: 20.16329322542463, Validation Loss: 22.450812816619873\n",
      "Epoch 117, Train Loss: 20.051723548344203, Validation Loss: 22.10965919494629\n",
      "Epoch 118, Train Loss: 19.8097630909511, Validation Loss: 22.243884563446045\n",
      "Epoch 119, Train Loss: 20.114868300301687, Validation Loss: 22.279350757598877\n",
      "Epoch 120, Train Loss: 19.962658677782333, Validation Loss: 22.498149871826172\n",
      "Epoch 121, Train Loss: 20.253430025918142, Validation Loss: 22.03648042678833\n",
      "Epoch 122, Train Loss: 19.956948825291224, Validation Loss: 22.75639533996582\n",
      "Epoch 123, Train Loss: 19.86877986363002, Validation Loss: 21.96707010269165\n",
      "Epoch 124, Train Loss: 19.683859484536306, Validation Loss: 21.93183135986328\n",
      "Epoch 125, Train Loss: 19.751838547842844, Validation Loss: 22.441608905792236\n",
      "Epoch 126, Train Loss: 20.235450744628906, Validation Loss: 22.083537578582764\n",
      "Epoch 127, Train Loss: 19.795966420854842, Validation Loss: 21.972642421722412\n",
      "Epoch 128, Train Loss: 19.839996610369003, Validation Loss: 22.1623272895813\n",
      "Epoch 129, Train Loss: 19.633924620492117, Validation Loss: 21.931870937347412\n",
      "Epoch 130, Train Loss: 19.647889069148473, Validation Loss: 21.82505512237549\n",
      "Epoch 131, Train Loss: 19.924120358058385, Validation Loss: 22.020901679992676\n",
      "Epoch 132, Train Loss: 20.337049552372523, Validation Loss: 22.53321123123169\n",
      "Epoch 133, Train Loss: 19.897478376116073, Validation Loss: 21.948136806488037\n",
      "Epoch 134, Train Loss: 19.80150638307844, Validation Loss: 22.312071323394775\n",
      "Epoch 135, Train Loss: 20.417166573660715, Validation Loss: 22.177913188934326\n",
      "Epoch 136, Train Loss: 19.99192292349679, Validation Loss: 21.88256072998047\n",
      "Epoch 137, Train Loss: 19.839085170200892, Validation Loss: 22.04552936553955\n",
      "Epoch 138, Train Loss: 20.000110353742325, Validation Loss: 21.971492290496826\n",
      "Epoch 139, Train Loss: 19.926651614052908, Validation Loss: 21.943470001220703\n",
      "Epoch 140, Train Loss: 19.69299282346453, Validation Loss: 22.028008460998535\n",
      "Epoch 141, Train Loss: 19.748839173998153, Validation Loss: 21.84617328643799\n",
      "Epoch 142, Train Loss: 20.25789669581822, Validation Loss: 23.111592769622803\n",
      "Epoch 143, Train Loss: 19.921115398406982, Validation Loss: 21.801422119140625\n",
      "Epoch 144, Train Loss: 19.902086802891322, Validation Loss: 22.587097644805908\n",
      "Epoch 145, Train Loss: 20.144169330596924, Validation Loss: 21.82460069656372\n",
      "Epoch 146, Train Loss: 19.692781720842635, Validation Loss: 22.577955722808838\n",
      "Epoch 147, Train Loss: 19.764761788504465, Validation Loss: 22.068574905395508\n",
      "Epoch 148, Train Loss: 19.552430357251847, Validation Loss: 21.928388595581055\n",
      "Epoch 149, Train Loss: 19.666166169302805, Validation Loss: 21.85851812362671\n",
      "Epoch 150, Train Loss: 19.77321801866804, Validation Loss: 22.503995418548584\n",
      "Epoch 151, Train Loss: 20.558560711996897, Validation Loss: 22.205448150634766\n",
      "Epoch 152, Train Loss: 19.83814866202218, Validation Loss: 21.96949863433838\n",
      "Epoch 153, Train Loss: 19.71993214743478, Validation Loss: 21.919875144958496\n",
      "Epoch 154, Train Loss: 19.65868595668248, Validation Loss: 21.89670991897583\n",
      "Epoch 155, Train Loss: 19.951047897338867, Validation Loss: 21.93038272857666\n",
      "Epoch 156, Train Loss: 19.556501933506556, Validation Loss: 21.94934844970703\n",
      "Epoch 157, Train Loss: 19.713757923671178, Validation Loss: 21.828699588775635\n",
      "Epoch 158, Train Loss: 19.8936482157026, Validation Loss: 23.258888244628906\n",
      "Epoch 159, Train Loss: 19.7464234488351, Validation Loss: 21.912950038909912\n",
      "Epoch 160, Train Loss: 19.718488012041366, Validation Loss: 21.833296298980713\n",
      "Epoch 161, Train Loss: 19.838863168443954, Validation Loss: 23.133974075317383\n",
      "Epoch 162, Train Loss: 20.293253217424667, Validation Loss: 21.800872325897217\n",
      "Epoch 163, Train Loss: 19.770873069763184, Validation Loss: 21.86291265487671\n",
      "Epoch 164, Train Loss: 19.724965027400426, Validation Loss: 22.005661964416504\n",
      "Epoch 165, Train Loss: 19.744572162628174, Validation Loss: 21.815431118011475\n",
      "Epoch 166, Train Loss: 19.887966769082205, Validation Loss: 22.539135456085205\n",
      "Epoch 167, Train Loss: 19.77325132914952, Validation Loss: 22.021002292633057\n",
      "Epoch 168, Train Loss: 19.54520811353411, Validation Loss: 21.90723705291748\n",
      "Epoch 169, Train Loss: 19.863369601113455, Validation Loss: 22.065324783325195\n",
      "Epoch 170, Train Loss: 19.988223484584264, Validation Loss: 21.880290031433105\n",
      "Epoch 171, Train Loss: 19.608559812818253, Validation Loss: 21.781388759613037\n",
      "Epoch 172, Train Loss: 19.600948810577393, Validation Loss: 21.747853755950928\n",
      "Epoch 173, Train Loss: 19.662983894348145, Validation Loss: 21.894254684448242\n",
      "Epoch 174, Train Loss: 19.77294703892299, Validation Loss: 22.10800838470459\n",
      "Epoch 175, Train Loss: 19.745559760502406, Validation Loss: 23.14809799194336\n",
      "Epoch 176, Train Loss: 20.07741825921195, Validation Loss: 22.146806240081787\n",
      "Epoch 177, Train Loss: 19.735450404030935, Validation Loss: 22.08741569519043\n",
      "Epoch 178, Train Loss: 19.58593988418579, Validation Loss: 21.72640371322632\n",
      "Epoch 179, Train Loss: 19.556647845676967, Validation Loss: 21.879793643951416\n",
      "Epoch 180, Train Loss: 19.486956936972483, Validation Loss: 21.786086559295654\n",
      "Epoch 181, Train Loss: 19.64688273838588, Validation Loss: 21.79573965072632\n",
      "Epoch 182, Train Loss: 19.64115728650774, Validation Loss: 22.11482810974121\n",
      "Epoch 183, Train Loss: 19.706692150660924, Validation Loss: 21.691275596618652\n",
      "Epoch 184, Train Loss: 19.97920220238822, Validation Loss: 22.524622917175293\n",
      "Epoch 185, Train Loss: 19.99906097139631, Validation Loss: 22.060690879821777\n",
      "Epoch 186, Train Loss: 19.625911440168107, Validation Loss: 21.880674839019775\n",
      "Epoch 187, Train Loss: 19.99799054009574, Validation Loss: 22.11967945098877\n",
      "Epoch 188, Train Loss: 19.699767317090714, Validation Loss: 21.7814884185791\n",
      "Epoch 189, Train Loss: 19.593374456678117, Validation Loss: 21.6786150932312\n",
      "Epoch 190, Train Loss: 19.62205900464739, Validation Loss: 22.356503009796143\n",
      "Epoch 191, Train Loss: 19.685544558933803, Validation Loss: 21.626265048980713\n",
      "Epoch 192, Train Loss: 19.514139311654226, Validation Loss: 21.686009883880615\n",
      "Epoch 193, Train Loss: 19.514860493796213, Validation Loss: 21.651851177215576\n",
      "Epoch 194, Train Loss: 19.50276987893241, Validation Loss: 21.759106159210205\n",
      "Epoch 195, Train Loss: 19.741359370095388, Validation Loss: 21.9201979637146\n",
      "Epoch 196, Train Loss: 19.551223141806467, Validation Loss: 22.24305248260498\n",
      "Epoch 197, Train Loss: 19.67955357687814, Validation Loss: 21.878979206085205\n",
      "Epoch 198, Train Loss: 19.70011411394392, Validation Loss: 21.79480504989624\n",
      "Epoch 199, Train Loss: 19.41955144064767, Validation Loss: 21.778184413909912\n",
      "Epoch 200, Train Loss: 19.56157296044486, Validation Loss: 21.85904550552368\n",
      "Epoch 201, Train Loss: 19.82803467341832, Validation Loss: 21.830291271209717\n",
      "Epoch 202, Train Loss: 19.57126385825021, Validation Loss: 21.897220134735107\n",
      "Epoch 203, Train Loss: 19.809476239340647, Validation Loss: 22.42080783843994\n",
      "Epoch 204, Train Loss: 20.1774582862854, Validation Loss: 21.756888389587402\n",
      "Epoch 205, Train Loss: 19.608819348471506, Validation Loss: 21.83945369720459\n",
      "Epoch 206, Train Loss: 19.480744089399064, Validation Loss: 21.84104061126709\n",
      "Epoch 207, Train Loss: 19.57157496043614, Validation Loss: 21.60983371734619\n",
      "Epoch 208, Train Loss: 19.615072386605398, Validation Loss: 21.905065059661865\n",
      "Epoch 209, Train Loss: 19.51962968281337, Validation Loss: 21.825583934783936\n",
      "Epoch 210, Train Loss: 19.479818207877024, Validation Loss: 22.57236623764038\n",
      "Epoch 211, Train Loss: 19.64707108906337, Validation Loss: 22.175516605377197\n",
      "Epoch 212, Train Loss: 19.551762308393204, Validation Loss: 22.028491973876953\n",
      "Epoch 213, Train Loss: 19.48665223802839, Validation Loss: 21.911848068237305\n",
      "Epoch 214, Train Loss: 19.45480912072318, Validation Loss: 21.55840253829956\n",
      "Epoch 215, Train Loss: 19.5598669392722, Validation Loss: 21.65944528579712\n",
      "Epoch 216, Train Loss: 19.89121001107352, Validation Loss: 22.290221214294434\n",
      "Epoch 217, Train Loss: 19.781411784035818, Validation Loss: 21.774250984191895\n",
      "Epoch 218, Train Loss: 19.56442710331508, Validation Loss: 21.63063383102417\n",
      "Epoch 219, Train Loss: 19.44982603618077, Validation Loss: 21.876115798950195\n",
      "Epoch 220, Train Loss: 19.55780553817749, Validation Loss: 21.78677749633789\n",
      "Epoch 221, Train Loss: 19.470378739493235, Validation Loss: 22.12291717529297\n",
      "Epoch 222, Train Loss: 19.64482545852661, Validation Loss: 21.802285194396973\n",
      "Epoch 223, Train Loss: 19.438072749546595, Validation Loss: 21.670867919921875\n",
      "Epoch 224, Train Loss: 19.349282264709473, Validation Loss: 21.608712196350098\n",
      "Epoch 225, Train Loss: 19.636607306344168, Validation Loss: 21.572068691253662\n",
      "Epoch 226, Train Loss: 19.505632196153915, Validation Loss: 22.921761512756348\n",
      "Epoch 227, Train Loss: 19.844920430864608, Validation Loss: 22.011985778808594\n",
      "Epoch 228, Train Loss: 19.966455255235946, Validation Loss: 21.923272132873535\n",
      "Epoch 229, Train Loss: 19.414698600769043, Validation Loss: 22.21048641204834\n",
      "Epoch 230, Train Loss: 19.845313140324183, Validation Loss: 21.68378448486328\n",
      "Epoch 231, Train Loss: 20.089356422424316, Validation Loss: 23.194921016693115\n",
      "Epoch 232, Train Loss: 20.228369644709996, Validation Loss: 21.749350547790527\n",
      "Epoch 233, Train Loss: 19.5386232648577, Validation Loss: 21.865732192993164\n",
      "Epoch 234, Train Loss: 19.52057933807373, Validation Loss: 21.71771478652954\n",
      "Epoch 235, Train Loss: 19.40509557723999, Validation Loss: 21.680254459381104\n",
      "Epoch 236, Train Loss: 19.476676532200404, Validation Loss: 21.798436641693115\n",
      "Epoch 237, Train Loss: 19.410508223942347, Validation Loss: 21.606541633605957\n",
      "Epoch 238, Train Loss: 19.340822560446604, Validation Loss: 21.578826904296875\n",
      "Epoch 239, Train Loss: 19.63821792602539, Validation Loss: 21.7492995262146\n",
      "Epoch 240, Train Loss: 19.372899532318115, Validation Loss: 21.860034465789795\n",
      "Epoch 241, Train Loss: 19.429420811789377, Validation Loss: 21.69110345840454\n",
      "Epoch 242, Train Loss: 19.513851438249862, Validation Loss: 21.967068195343018\n",
      "Epoch 243, Train Loss: 19.550083228519984, Validation Loss: 21.731927394866943\n",
      "Epoch 244, Train Loss: 19.440390382494247, Validation Loss: 21.58806610107422\n",
      "Epoch 245, Train Loss: 19.63766016278948, Validation Loss: 21.736775875091553\n",
      "Epoch 246, Train Loss: 20.061618055616105, Validation Loss: 22.021709442138672\n",
      "Epoch 247, Train Loss: 19.60642126628331, Validation Loss: 21.668372631072998\n",
      "Epoch 248, Train Loss: 19.4587436403547, Validation Loss: 21.650508880615234\n",
      "Epoch 249, Train Loss: 19.387192249298096, Validation Loss: 21.58432388305664\n",
      "Epoch 250, Train Loss: 19.787790162222727, Validation Loss: 22.216418743133545\n",
      "Epoch 251, Train Loss: 19.690048967088973, Validation Loss: 22.17587661743164\n",
      "Epoch 252, Train Loss: 19.680243696485245, Validation Loss: 21.478506088256836\n",
      "Epoch 253, Train Loss: 19.345627852848597, Validation Loss: 21.545586109161377\n",
      "Epoch 254, Train Loss: 19.318966116224015, Validation Loss: 21.574239253997803\n",
      "Epoch 255, Train Loss: 19.51955120904105, Validation Loss: 21.60200786590576\n",
      "Epoch 256, Train Loss: 19.60399293899536, Validation Loss: 21.611306190490723\n",
      "Epoch 257, Train Loss: 19.364010197775706, Validation Loss: 22.12479019165039\n",
      "Epoch 258, Train Loss: 19.338787487574987, Validation Loss: 21.567872047424316\n",
      "Epoch 259, Train Loss: 19.395846911839076, Validation Loss: 21.62696123123169\n",
      "Epoch 260, Train Loss: 19.394683156694686, Validation Loss: 21.630619049072266\n",
      "Epoch 261, Train Loss: 19.43646056311471, Validation Loss: 22.63035011291504\n",
      "Epoch 262, Train Loss: 19.54577398300171, Validation Loss: 21.74709177017212\n",
      "Epoch 263, Train Loss: 19.53360230582101, Validation Loss: 21.674095153808594\n",
      "Epoch 264, Train Loss: 19.4649761063712, Validation Loss: 21.616039752960205\n",
      "Epoch 265, Train Loss: 19.30563395363944, Validation Loss: 21.689430236816406\n",
      "Epoch 266, Train Loss: 19.41400691441127, Validation Loss: 21.661736965179443\n",
      "Epoch 267, Train Loss: 19.319336891174316, Validation Loss: 21.608003616333008\n",
      "Epoch 268, Train Loss: 19.33835792541504, Validation Loss: 21.635575771331787\n",
      "Epoch 269, Train Loss: 19.707585947854177, Validation Loss: 21.54955291748047\n",
      "Epoch 270, Train Loss: 19.240872655596053, Validation Loss: 21.47549819946289\n",
      "Epoch 271, Train Loss: 19.36734880719866, Validation Loss: 21.430642127990723\n",
      "Epoch 272, Train Loss: 19.291116986955917, Validation Loss: 21.436365604400635\n",
      "Epoch 273, Train Loss: 19.48753390993391, Validation Loss: 21.580027103424072\n",
      "Epoch 274, Train Loss: 19.701009069170272, Validation Loss: 22.601818561553955\n",
      "Epoch 275, Train Loss: 19.44767475128174, Validation Loss: 21.601173877716064\n",
      "Epoch 276, Train Loss: 19.382641860416957, Validation Loss: 21.653940200805664\n",
      "Epoch 277, Train Loss: 19.310421807425364, Validation Loss: 21.53518009185791\n",
      "Epoch 278, Train Loss: 19.337808540889196, Validation Loss: 21.49529457092285\n",
      "Epoch 279, Train Loss: 19.39232213156564, Validation Loss: 21.78202533721924\n",
      "Epoch 280, Train Loss: 19.538628918784006, Validation Loss: 21.65429449081421\n",
      "Epoch 281, Train Loss: 19.334976060049875, Validation Loss: 21.539135932922363\n",
      "Epoch 282, Train Loss: 19.22636236463274, Validation Loss: 22.22447443008423\n",
      "Epoch 283, Train Loss: 19.570817402430944, Validation Loss: 21.541022300720215\n",
      "Epoch 284, Train Loss: 19.36245822906494, Validation Loss: 21.45264434814453\n",
      "Epoch 285, Train Loss: 19.333677291870117, Validation Loss: 21.851945877075195\n",
      "Epoch 286, Train Loss: 19.351103373936244, Validation Loss: 21.632190227508545\n",
      "Epoch 287, Train Loss: 19.509615761893137, Validation Loss: 21.539016723632812\n",
      "Epoch 288, Train Loss: 19.383323328835623, Validation Loss: 21.945937156677246\n",
      "Epoch 289, Train Loss: 19.48807988848005, Validation Loss: 21.579918384552002\n",
      "Epoch 290, Train Loss: 19.2464314188276, Validation Loss: 21.72022008895874\n",
      "Epoch 291, Train Loss: 19.237178870609828, Validation Loss: 21.48280954360962\n",
      "Epoch 292, Train Loss: 19.457583699907577, Validation Loss: 21.830172538757324\n",
      "Epoch 293, Train Loss: 19.49285023553031, Validation Loss: 21.6874041557312\n",
      "Epoch 294, Train Loss: 19.42301539012364, Validation Loss: 21.9846453666687\n",
      "Epoch 295, Train Loss: 19.3796101297651, Validation Loss: 21.620662212371826\n",
      "Epoch 296, Train Loss: 19.336431435176305, Validation Loss: 21.479636192321777\n",
      "Epoch 297, Train Loss: 19.180887971605575, Validation Loss: 21.381640434265137\n",
      "Epoch 298, Train Loss: 19.48476573399135, Validation Loss: 21.56113576889038\n",
      "Epoch 299, Train Loss: 19.28382866723197, Validation Loss: 21.582231521606445\n",
      "Epoch 300, Train Loss: 19.21652623585292, Validation Loss: 21.606789112091064\n",
      "Epoch 301, Train Loss: 19.29789468220302, Validation Loss: 21.362425327301025\n",
      "Epoch 302, Train Loss: 19.32763828550066, Validation Loss: 21.456204891204834\n",
      "Epoch 303, Train Loss: 19.600486210414342, Validation Loss: 21.469536304473877\n",
      "Epoch 304, Train Loss: 19.4206120627267, Validation Loss: 21.61327028274536\n",
      "Epoch 305, Train Loss: 19.335270881652832, Validation Loss: 21.560964107513428\n",
      "Epoch 306, Train Loss: 19.223199708121165, Validation Loss: 21.66767930984497\n",
      "Epoch 307, Train Loss: 19.602713312421525, Validation Loss: 21.605135917663574\n",
      "Epoch 308, Train Loss: 19.403464112962997, Validation Loss: 21.40385913848877\n",
      "Epoch 309, Train Loss: 19.275887284960067, Validation Loss: 21.41483974456787\n",
      "Epoch 310, Train Loss: 19.303012711661204, Validation Loss: 22.010021686553955\n",
      "Epoch 311, Train Loss: 19.667967523847306, Validation Loss: 21.58666229248047\n",
      "Epoch 312, Train Loss: 19.397991725376674, Validation Loss: 21.44821262359619\n",
      "Epoch 313, Train Loss: 19.172223840441024, Validation Loss: 21.81175661087036\n",
      "Epoch 314, Train Loss: 19.23079184123448, Validation Loss: 21.462724685668945\n",
      "Epoch 315, Train Loss: 19.51710687364851, Validation Loss: 21.337480545043945\n",
      "Epoch 316, Train Loss: 19.17155953816005, Validation Loss: 21.312658309936523\n",
      "Epoch 317, Train Loss: 19.274943351745605, Validation Loss: 21.408814430236816\n",
      "Epoch 318, Train Loss: 19.12550319944109, Validation Loss: 21.34985113143921\n",
      "Epoch 319, Train Loss: 19.29558413369315, Validation Loss: 21.41364336013794\n",
      "Epoch 320, Train Loss: 19.570785113743373, Validation Loss: 21.48279619216919\n",
      "Epoch 321, Train Loss: 19.192583424704416, Validation Loss: 21.405505180358887\n",
      "Epoch 322, Train Loss: 19.170283998761857, Validation Loss: 21.37889575958252\n",
      "Epoch 323, Train Loss: 19.203189509255544, Validation Loss: 21.361145496368408\n",
      "Epoch 324, Train Loss: 19.345570632389613, Validation Loss: 21.587424755096436\n",
      "Epoch 325, Train Loss: 19.18837765284947, Validation Loss: 21.381577014923096\n",
      "Epoch 326, Train Loss: 19.248637403760636, Validation Loss: 21.591405391693115\n",
      "Epoch 327, Train Loss: 19.275288922446116, Validation Loss: 21.64183235168457\n",
      "Epoch 328, Train Loss: 19.32293278830392, Validation Loss: 21.345205783843994\n",
      "Epoch 329, Train Loss: 19.28818348475865, Validation Loss: 21.256908416748047\n",
      "Epoch 330, Train Loss: 19.137215001242502, Validation Loss: 21.945086002349854\n",
      "Epoch 331, Train Loss: 19.220898968832834, Validation Loss: 21.85939311981201\n",
      "Epoch 332, Train Loss: 19.185022558484757, Validation Loss: 21.235453128814697\n",
      "Epoch 333, Train Loss: 19.09142357962472, Validation Loss: 21.373304843902588\n",
      "Epoch 334, Train Loss: 19.175746849605016, Validation Loss: 21.72471284866333\n",
      "Epoch 335, Train Loss: 20.29041474206107, Validation Loss: 21.55914545059204\n",
      "Epoch 336, Train Loss: 19.475252560206822, Validation Loss: 21.517262935638428\n",
      "Epoch 337, Train Loss: 19.24463047300066, Validation Loss: 21.513322353363037\n",
      "Epoch 338, Train Loss: 19.54060431889125, Validation Loss: 21.861303329467773\n",
      "Epoch 339, Train Loss: 19.33872617994036, Validation Loss: 21.323880672454834\n",
      "Epoch 340, Train Loss: 19.16127266202654, Validation Loss: 21.41808557510376\n",
      "Epoch 341, Train Loss: 19.105374881199428, Validation Loss: 21.312710285186768\n",
      "Epoch 342, Train Loss: 19.12577792576381, Validation Loss: 21.4788179397583\n",
      "Epoch 343, Train Loss: 19.422848633357457, Validation Loss: 21.71268939971924\n",
      "Epoch 344, Train Loss: 19.23945243018014, Validation Loss: 21.816450595855713\n",
      "Epoch 345, Train Loss: 19.49662705830165, Validation Loss: 21.90516471862793\n",
      "Epoch 346, Train Loss: 19.162275178091868, Validation Loss: 21.30275297164917\n",
      "Epoch 347, Train Loss: 19.160957677023752, Validation Loss: 21.282116413116455\n",
      "Epoch 348, Train Loss: 19.181105886186874, Validation Loss: 21.409648895263672\n",
      "Epoch 349, Train Loss: 19.35324341910226, Validation Loss: 21.703837871551514\n",
      "Epoch 350, Train Loss: 19.160328592572892, Validation Loss: 21.668330669403076\n",
      "Epoch 351, Train Loss: 19.248912470681326, Validation Loss: 21.37183141708374\n",
      "Epoch 352, Train Loss: 19.048461300986155, Validation Loss: 21.30083990097046\n",
      "Epoch 353, Train Loss: 19.101250341960363, Validation Loss: 21.18208122253418\n",
      "Epoch 354, Train Loss: 19.256379536220006, Validation Loss: 21.85982370376587\n",
      "Epoch 355, Train Loss: 19.2611038344247, Validation Loss: 21.929214477539062\n",
      "Epoch 356, Train Loss: 19.301238196236746, Validation Loss: 21.51015615463257\n",
      "Epoch 357, Train Loss: 19.15799883433751, Validation Loss: 21.710800647735596\n",
      "Epoch 358, Train Loss: 19.111587592533656, Validation Loss: 21.21295738220215\n",
      "Epoch 359, Train Loss: 19.152321134294784, Validation Loss: 21.302320957183838\n",
      "Epoch 360, Train Loss: 19.09837681906564, Validation Loss: 21.42830467224121\n",
      "Epoch 361, Train Loss: 19.163124425070627, Validation Loss: 21.210087776184082\n",
      "Epoch 362, Train Loss: 19.14776223046439, Validation Loss: 21.562131881713867\n",
      "Epoch 363, Train Loss: 19.124997752053396, Validation Loss: 21.54376220703125\n",
      "Epoch 364, Train Loss: 19.246834414345876, Validation Loss: 21.208983898162842\n",
      "Epoch 365, Train Loss: 19.296292441231863, Validation Loss: 21.32329273223877\n",
      "Epoch 366, Train Loss: 19.060501439230784, Validation Loss: 21.394166469573975\n",
      "Epoch 367, Train Loss: 19.021260125296457, Validation Loss: 21.12571144104004\n",
      "Epoch 368, Train Loss: 19.098619529179164, Validation Loss: 21.083505630493164\n",
      "Epoch 369, Train Loss: 18.976930413927352, Validation Loss: 21.049355030059814\n",
      "Epoch 370, Train Loss: 19.115675108773367, Validation Loss: 21.593005657196045\n",
      "Epoch 371, Train Loss: 19.367273330688477, Validation Loss: 23.463881492614746\n",
      "Epoch 372, Train Loss: 19.47296006338937, Validation Loss: 21.32349967956543\n",
      "Epoch 373, Train Loss: 19.09211734363011, Validation Loss: 21.149620056152344\n",
      "Epoch 374, Train Loss: 19.265939780644008, Validation Loss: 21.31416082382202\n",
      "Epoch 375, Train Loss: 19.260838781084335, Validation Loss: 21.19958782196045\n",
      "Epoch 376, Train Loss: 18.985033784593856, Validation Loss: 21.052310943603516\n",
      "Epoch 377, Train Loss: 19.17929697036743, Validation Loss: 21.099987030029297\n",
      "Epoch 378, Train Loss: 19.09316532952445, Validation Loss: 21.13794994354248\n",
      "Epoch 379, Train Loss: 19.01806013924735, Validation Loss: 21.543842792510986\n",
      "Epoch 380, Train Loss: 19.04671941484724, Validation Loss: 21.13978147506714\n",
      "Epoch 381, Train Loss: 18.965095724378312, Validation Loss: 21.290700912475586\n",
      "Epoch 382, Train Loss: 19.478005273001536, Validation Loss: 21.790081024169922\n",
      "Epoch 383, Train Loss: 19.160229001726425, Validation Loss: 21.153740406036377\n",
      "Epoch 384, Train Loss: 18.98325334276472, Validation Loss: 21.114789485931396\n",
      "Epoch 385, Train Loss: 19.18896538870675, Validation Loss: 21.710020065307617\n",
      "Epoch 386, Train Loss: 19.13623673575265, Validation Loss: 21.196138858795166\n",
      "Epoch 387, Train Loss: 19.138546330588206, Validation Loss: 21.4713077545166\n",
      "Epoch 388, Train Loss: 19.329840932573592, Validation Loss: 21.571349143981934\n",
      "Epoch 389, Train Loss: 19.24413456235613, Validation Loss: 21.120517253875732\n",
      "Epoch 390, Train Loss: 19.19505180631365, Validation Loss: 21.109939575195312\n",
      "Epoch 391, Train Loss: 19.1325808933803, Validation Loss: 21.135314464569092\n",
      "Epoch 392, Train Loss: 19.058181285858154, Validation Loss: 21.0445876121521\n",
      "Epoch 393, Train Loss: 18.938736779349192, Validation Loss: 21.072283267974854\n",
      "Epoch 394, Train Loss: 19.0395690373012, Validation Loss: 21.235397338867188\n",
      "Epoch 395, Train Loss: 19.190896102360316, Validation Loss: 21.14102554321289\n",
      "Epoch 396, Train Loss: 19.054223946162633, Validation Loss: 21.149032592773438\n",
      "Epoch 397, Train Loss: 18.99593142100743, Validation Loss: 21.005820274353027\n",
      "Epoch 398, Train Loss: 18.907031399863108, Validation Loss: 21.050885677337646\n",
      "Epoch 399, Train Loss: 18.911633287157333, Validation Loss: 21.053627490997314\n",
      "Epoch 400, Train Loss: 19.019065652574813, Validation Loss: 20.969067573547363\n",
      "Epoch 401, Train Loss: 18.925355093819753, Validation Loss: 21.01546621322632\n",
      "Epoch 402, Train Loss: 19.221222945622035, Validation Loss: 21.360276222229004\n",
      "Epoch 403, Train Loss: 19.183377197810582, Validation Loss: 20.983256816864014\n",
      "Epoch 404, Train Loss: 19.05871800013951, Validation Loss: 21.025487899780273\n",
      "Epoch 405, Train Loss: 19.024977684020996, Validation Loss: 21.292564392089844\n",
      "Epoch 406, Train Loss: 18.935570240020752, Validation Loss: 21.08331060409546\n",
      "Epoch 407, Train Loss: 19.151198387145996, Validation Loss: 21.247206211090088\n",
      "Epoch 408, Train Loss: 19.078453949519567, Validation Loss: 21.062783241271973\n",
      "Epoch 409, Train Loss: 18.98162249156407, Validation Loss: 21.1471266746521\n",
      "Epoch 410, Train Loss: 18.964596816471644, Validation Loss: 21.260124683380127\n",
      "Epoch 411, Train Loss: 18.98992988041469, Validation Loss: 21.019295692443848\n",
      "Epoch 412, Train Loss: 18.89303289140974, Validation Loss: 21.481185913085938\n",
      "Epoch 413, Train Loss: 19.078479494367325, Validation Loss: 21.066046237945557\n",
      "Epoch 414, Train Loss: 19.018734046391078, Validation Loss: 21.276505947113037\n",
      "Epoch 415, Train Loss: 19.395068645477295, Validation Loss: 21.017889976501465\n",
      "Epoch 416, Train Loss: 19.05722052710397, Validation Loss: 20.991597175598145\n",
      "Epoch 417, Train Loss: 18.932143688201904, Validation Loss: 20.926530838012695\n",
      "Epoch 418, Train Loss: 18.953438963208878, Validation Loss: 21.12747859954834\n",
      "Epoch 419, Train Loss: 18.910827228001185, Validation Loss: 21.163783073425293\n",
      "Epoch 420, Train Loss: 18.989828177860804, Validation Loss: 20.975879192352295\n",
      "Epoch 421, Train Loss: 19.01227072307042, Validation Loss: 21.043309688568115\n",
      "Epoch 422, Train Loss: 18.86976616723197, Validation Loss: 20.92375373840332\n",
      "Epoch 423, Train Loss: 18.939283439091273, Validation Loss: 21.053942680358887\n",
      "Epoch 424, Train Loss: 18.875590596880233, Validation Loss: 20.950734615325928\n",
      "Epoch 425, Train Loss: 18.93266487121582, Validation Loss: 21.251952648162842\n",
      "Epoch 426, Train Loss: 19.041545254843577, Validation Loss: 21.06194496154785\n",
      "Epoch 427, Train Loss: 19.01645748955863, Validation Loss: 20.991299152374268\n",
      "Epoch 428, Train Loss: 19.184428419385636, Validation Loss: 21.485326766967773\n",
      "Epoch 429, Train Loss: 19.08737870625087, Validation Loss: 21.03441095352173\n",
      "Epoch 430, Train Loss: 18.95975140162877, Validation Loss: 21.267616271972656\n",
      "Epoch 431, Train Loss: 19.023816994258336, Validation Loss: 21.040325164794922\n",
      "Epoch 432, Train Loss: 18.975069046020508, Validation Loss: 21.661351203918457\n",
      "Epoch 433, Train Loss: 19.133948053632462, Validation Loss: 20.960022449493408\n",
      "Epoch 434, Train Loss: 18.881410598754883, Validation Loss: 20.911357402801514\n",
      "Epoch 435, Train Loss: 18.80566426685878, Validation Loss: 20.926087379455566\n",
      "Epoch 436, Train Loss: 18.91924830845424, Validation Loss: 21.09278917312622\n",
      "Epoch 437, Train Loss: 18.925896440233505, Validation Loss: 21.25767230987549\n",
      "Epoch 438, Train Loss: 19.002658912113734, Validation Loss: 21.495323181152344\n",
      "Epoch 439, Train Loss: 19.13686616080148, Validation Loss: 21.236888885498047\n",
      "Epoch 440, Train Loss: 19.11613314492362, Validation Loss: 20.956719398498535\n",
      "Epoch 441, Train Loss: 19.041870628084457, Validation Loss: 21.080322742462158\n",
      "Epoch 442, Train Loss: 18.937606402805873, Validation Loss: 21.444241046905518\n",
      "Epoch 443, Train Loss: 19.06520468848092, Validation Loss: 20.979870796203613\n",
      "Epoch 444, Train Loss: 19.026637145451136, Validation Loss: 21.723771572113037\n",
      "Epoch 445, Train Loss: 19.51341485977173, Validation Loss: 21.141953468322754\n",
      "Epoch 446, Train Loss: 18.94657611846924, Validation Loss: 21.328284740447998\n",
      "Epoch 447, Train Loss: 18.915537084851945, Validation Loss: 20.99409580230713\n",
      "Epoch 448, Train Loss: 18.895788397107804, Validation Loss: 21.317625045776367\n",
      "Epoch 449, Train Loss: 19.109994070870535, Validation Loss: 20.887539386749268\n",
      "Epoch 450, Train Loss: 18.970426286969865, Validation Loss: 21.09883403778076\n",
      "Epoch 451, Train Loss: 18.904933316367014, Validation Loss: 21.023375511169434\n",
      "Epoch 452, Train Loss: 18.961490290505544, Validation Loss: 21.007272243499756\n",
      "Epoch 453, Train Loss: 19.083438192095077, Validation Loss: 21.501843452453613\n",
      "Epoch 454, Train Loss: 19.369800976344518, Validation Loss: 21.33690309524536\n",
      "Epoch 455, Train Loss: 19.036208493368967, Validation Loss: 20.901467323303223\n",
      "Epoch 456, Train Loss: 18.943525995526993, Validation Loss: 21.010403156280518\n",
      "Epoch 457, Train Loss: 19.02711112158639, Validation Loss: 20.978853225708008\n",
      "Epoch 458, Train Loss: 18.97053677695138, Validation Loss: 20.95224380493164\n",
      "Epoch 459, Train Loss: 19.096713747297013, Validation Loss: 20.930127143859863\n",
      "Epoch 460, Train Loss: 19.008294105529785, Validation Loss: 21.526444911956787\n",
      "Epoch 461, Train Loss: 19.410026618412562, Validation Loss: 21.51535701751709\n",
      "Epoch 462, Train Loss: 19.0886173248291, Validation Loss: 21.018949508666992\n",
      "Epoch 463, Train Loss: 18.980008397783553, Validation Loss: 20.94270658493042\n",
      "Epoch 464, Train Loss: 18.993599278586252, Validation Loss: 21.18551540374756\n",
      "Epoch 465, Train Loss: 18.950388976505824, Validation Loss: 20.997002124786377\n",
      "Epoch 466, Train Loss: 19.023399216788157, Validation Loss: 20.92824411392212\n",
      "Epoch 467, Train Loss: 18.88107933316912, Validation Loss: 20.95125436782837\n",
      "Epoch 468, Train Loss: 19.15284620012556, Validation Loss: 21.586583137512207\n",
      "Epoch 469, Train Loss: 19.226704324994767, Validation Loss: 21.02866506576538\n",
      "Epoch 470, Train Loss: 18.87260123661586, Validation Loss: 21.01324701309204\n",
      "Epoch 471, Train Loss: 18.859476770673478, Validation Loss: 21.09301519393921\n",
      "Epoch 472, Train Loss: 18.960380486079625, Validation Loss: 20.910947799682617\n",
      "Epoch 473, Train Loss: 18.949341705867223, Validation Loss: 21.24754047393799\n",
      "Epoch 474, Train Loss: 19.157129764556885, Validation Loss: 20.98472261428833\n",
      "Epoch 475, Train Loss: 18.937566416604177, Validation Loss: 21.18333387374878\n",
      "Epoch 476, Train Loss: 18.98750557218279, Validation Loss: 20.99323034286499\n",
      "Epoch 477, Train Loss: 18.941211564200266, Validation Loss: 21.022918701171875\n",
      "Epoch 478, Train Loss: 18.893446717943466, Validation Loss: 20.85955572128296\n",
      "Epoch 479, Train Loss: 18.869606835501536, Validation Loss: 20.957510471343994\n",
      "Epoch 480, Train Loss: 19.108300822121755, Validation Loss: 21.102584838867188\n",
      "Epoch 481, Train Loss: 19.04245035988944, Validation Loss: 21.047510147094727\n",
      "Epoch 482, Train Loss: 19.109652042388916, Validation Loss: 20.895856380462646\n",
      "Epoch 483, Train Loss: 18.842142990657262, Validation Loss: 20.88244867324829\n",
      "Epoch 484, Train Loss: 19.053396156855992, Validation Loss: 21.02325439453125\n",
      "Epoch 485, Train Loss: 19.08550889151437, Validation Loss: 21.024719715118408\n",
      "Epoch 486, Train Loss: 18.935059683663503, Validation Loss: 20.91576862335205\n",
      "Epoch 487, Train Loss: 19.179770401545934, Validation Loss: 21.41806125640869\n",
      "Epoch 488, Train Loss: 18.906695434025355, Validation Loss: 20.93476963043213\n",
      "Epoch 489, Train Loss: 18.937460354396276, Validation Loss: 21.401672840118408\n",
      "Epoch 490, Train Loss: 19.0450804574149, Validation Loss: 21.24135971069336\n",
      "Epoch 491, Train Loss: 19.226077216012136, Validation Loss: 21.361472606658936\n",
      "Epoch 492, Train Loss: 18.963486126491002, Validation Loss: 21.282389163970947\n",
      "Epoch 493, Train Loss: 18.924004077911377, Validation Loss: 20.92685031890869\n",
      "Epoch 494, Train Loss: 18.926965509142196, Validation Loss: 20.909615993499756\n",
      "Epoch 495, Train Loss: 19.029011522020614, Validation Loss: 21.51169204711914\n",
      "Epoch 496, Train Loss: 18.938212190355575, Validation Loss: 20.93175172805786\n",
      "Epoch 497, Train Loss: 18.94134031023298, Validation Loss: 21.141210079193115\n",
      "Epoch 498, Train Loss: 18.999566078186035, Validation Loss: 20.839948177337646\n",
      "Epoch 499, Train Loss: 18.887480190822057, Validation Loss: 20.96051836013794\n",
      "Epoch 500, Train Loss: 18.887258665902273, Validation Loss: 20.886470794677734\n",
      "Epoch 501, Train Loss: 18.87943744659424, Validation Loss: 20.850502490997314\n",
      "Epoch 502, Train Loss: 19.092751026153564, Validation Loss: 21.17771053314209\n",
      "Epoch 503, Train Loss: 19.012116636548722, Validation Loss: 20.91463613510132\n",
      "Epoch 504, Train Loss: 18.87021960530962, Validation Loss: 20.948763847351074\n",
      "Epoch 505, Train Loss: 18.981195654187882, Validation Loss: 20.896389484405518\n",
      "Epoch 506, Train Loss: 19.099284512656077, Validation Loss: 21.0339093208313\n",
      "Epoch 507, Train Loss: 18.87320831843785, Validation Loss: 20.871078968048096\n",
      "Epoch 508, Train Loss: 18.873162746429443, Validation Loss: 20.920443058013916\n",
      "Epoch 509, Train Loss: 18.891445091792516, Validation Loss: 21.042303562164307\n",
      "Epoch 510, Train Loss: 19.069257599966868, Validation Loss: 21.024807453155518\n",
      "Epoch 511, Train Loss: 19.13223361968994, Validation Loss: 21.126648902893066\n",
      "Epoch 512, Train Loss: 19.253381933484757, Validation Loss: 20.930455684661865\n",
      "Epoch 513, Train Loss: 19.00782026563372, Validation Loss: 21.044841289520264\n",
      "Epoch 514, Train Loss: 18.853940418788365, Validation Loss: 21.031894207000732\n",
      "Epoch 515, Train Loss: 19.019508498055593, Validation Loss: 21.035913944244385\n",
      "Epoch 516, Train Loss: 18.957847390856063, Validation Loss: 20.8519868850708\n",
      "Epoch 517, Train Loss: 18.820609092712402, Validation Loss: 20.832812309265137\n",
      "Epoch 518, Train Loss: 19.049572195325577, Validation Loss: 21.55368661880493\n",
      "Epoch 519, Train Loss: 18.930688653673446, Validation Loss: 20.84676218032837\n",
      "Epoch 520, Train Loss: 18.855798993791854, Validation Loss: 21.05147361755371\n",
      "Epoch 521, Train Loss: 19.077653339930944, Validation Loss: 21.013041019439697\n",
      "Epoch 522, Train Loss: 18.93591090611049, Validation Loss: 20.86580753326416\n",
      "Epoch 523, Train Loss: 18.809163638523646, Validation Loss: 20.99934482574463\n",
      "Epoch 524, Train Loss: 18.906092848096574, Validation Loss: 20.89548969268799\n",
      "Epoch 525, Train Loss: 18.896674701145717, Validation Loss: 21.790860652923584\n",
      "Epoch 526, Train Loss: 19.269482953207834, Validation Loss: 21.002118587493896\n",
      "Epoch 527, Train Loss: 19.25669036592756, Validation Loss: 21.581632137298584\n",
      "Epoch 528, Train Loss: 19.066029480525426, Validation Loss: 21.051445960998535\n",
      "Epoch 529, Train Loss: 19.059879779815674, Validation Loss: 20.913455963134766\n",
      "Epoch 530, Train Loss: 18.97649839946202, Validation Loss: 20.980793476104736\n",
      "Epoch 531, Train Loss: 19.412928989955358, Validation Loss: 22.733275890350342\n",
      "Epoch 532, Train Loss: 20.24902888706752, Validation Loss: 21.92449378967285\n",
      "Epoch 533, Train Loss: 19.355489798954554, Validation Loss: 21.321409702301025\n",
      "Epoch 534, Train Loss: 19.053711346217565, Validation Loss: 20.95988702774048\n",
      "Epoch 535, Train Loss: 18.857439926692418, Validation Loss: 21.114152908325195\n",
      "Epoch 536, Train Loss: 18.86383363178798, Validation Loss: 20.910276412963867\n",
      "Epoch 537, Train Loss: 18.83239303316389, Validation Loss: 20.92632484436035\n",
      "Epoch 538, Train Loss: 19.198664937700546, Validation Loss: 20.909030437469482\n",
      "Epoch 539, Train Loss: 18.957066331590926, Validation Loss: 20.961047172546387\n",
      "Epoch 540, Train Loss: 18.878953320639475, Validation Loss: 20.843209743499756\n",
      "Epoch 541, Train Loss: 18.831395353589738, Validation Loss: 20.823588371276855\n",
      "Epoch 542, Train Loss: 19.02229724611555, Validation Loss: 20.980255603790283\n",
      "Epoch 543, Train Loss: 18.824015344892228, Validation Loss: 20.821404933929443\n",
      "Epoch 544, Train Loss: 18.831311021532333, Validation Loss: 20.94526481628418\n",
      "Epoch 545, Train Loss: 18.92028740474156, Validation Loss: 20.855183601379395\n",
      "Epoch 546, Train Loss: 18.852690764835902, Validation Loss: 20.97818660736084\n",
      "Epoch 547, Train Loss: 18.96197816303798, Validation Loss: 20.844334602355957\n",
      "Epoch 548, Train Loss: 19.224940913064138, Validation Loss: 20.937965393066406\n",
      "Epoch 549, Train Loss: 18.869869095938547, Validation Loss: 20.876259326934814\n",
      "Epoch 550, Train Loss: 18.988578251429967, Validation Loss: 20.963085174560547\n",
      "Epoch 551, Train Loss: 18.889860016959055, Validation Loss: 20.82492160797119\n",
      "Epoch 552, Train Loss: 18.84058448246547, Validation Loss: 21.430561542510986\n",
      "Epoch 553, Train Loss: 18.981068338666642, Validation Loss: 21.093931198120117\n",
      "Epoch 554, Train Loss: 18.867823464529856, Validation Loss: 21.079140663146973\n",
      "Epoch 555, Train Loss: 19.080058302198136, Validation Loss: 21.266039848327637\n",
      "Epoch 556, Train Loss: 18.998142651149205, Validation Loss: 21.0262770652771\n",
      "Epoch 557, Train Loss: 19.318273953029088, Validation Loss: 21.107892513275146\n",
      "Epoch 558, Train Loss: 18.86842700413295, Validation Loss: 20.96678400039673\n",
      "Epoch 559, Train Loss: 19.123534883771622, Validation Loss: 21.022568702697754\n",
      "Epoch 560, Train Loss: 19.100386960165842, Validation Loss: 20.96150016784668\n",
      "Epoch 561, Train Loss: 18.989781992776052, Validation Loss: 20.9929141998291\n",
      "Epoch 562, Train Loss: 18.9391234261649, Validation Loss: 20.815109729766846\n",
      "Epoch 563, Train Loss: 18.904030527387345, Validation Loss: 20.895606994628906\n",
      "Epoch 564, Train Loss: 18.967817374638148, Validation Loss: 20.936171054840088\n",
      "Epoch 565, Train Loss: 18.919229643685476, Validation Loss: 20.82736825942993\n",
      "Epoch 566, Train Loss: 18.983447687966482, Validation Loss: 20.875624179840088\n",
      "Epoch 567, Train Loss: 18.987583228519984, Validation Loss: 21.69883680343628\n",
      "Epoch 568, Train Loss: 19.099505969456263, Validation Loss: 21.102920532226562\n",
      "Epoch 569, Train Loss: 19.013678550720215, Validation Loss: 20.86415147781372\n",
      "Epoch 570, Train Loss: 18.93208646774292, Validation Loss: 20.991506576538086\n",
      "Epoch 571, Train Loss: 18.89564868382045, Validation Loss: 20.82307195663452\n",
      "Epoch 572, Train Loss: 18.94177750178746, Validation Loss: 20.889673709869385\n",
      "Epoch 573, Train Loss: 19.12195192064558, Validation Loss: 21.0692400932312\n",
      "Epoch 574, Train Loss: 19.100692953382218, Validation Loss: 20.9196834564209\n",
      "Epoch 575, Train Loss: 19.014946801321848, Validation Loss: 20.930784225463867\n",
      "Epoch 576, Train Loss: 18.812803200313024, Validation Loss: 20.95277976989746\n",
      "Epoch 577, Train Loss: 18.90224415915353, Validation Loss: 20.819932460784912\n",
      "Epoch 578, Train Loss: 19.0452755519322, Validation Loss: 21.01587152481079\n",
      "Epoch 579, Train Loss: 19.112743309565953, Validation Loss: 20.96602153778076\n",
      "Epoch 580, Train Loss: 19.003353118896484, Validation Loss: 21.22193717956543\n",
      "Epoch 581, Train Loss: 19.37758527483259, Validation Loss: 21.1708984375\n",
      "Epoch 582, Train Loss: 18.910354818616593, Validation Loss: 20.98022985458374\n",
      "Epoch 583, Train Loss: 18.791533538273402, Validation Loss: 21.37999439239502\n",
      "Epoch 584, Train Loss: 18.977162633623397, Validation Loss: 20.77964973449707\n",
      "Epoch 585, Train Loss: 18.97999198096139, Validation Loss: 21.067311763763428\n",
      "Epoch 586, Train Loss: 18.89575011389596, Validation Loss: 20.812021255493164\n",
      "Epoch 587, Train Loss: 18.870344434465682, Validation Loss: 21.073496341705322\n",
      "Epoch 588, Train Loss: 18.904404708317347, Validation Loss: 20.8626651763916\n",
      "Epoch 589, Train Loss: 18.80739872796195, Validation Loss: 20.80244541168213\n",
      "Epoch 590, Train Loss: 18.90487916128976, Validation Loss: 20.892935752868652\n",
      "Epoch 591, Train Loss: 19.15618630817958, Validation Loss: 21.290008068084717\n",
      "Epoch 592, Train Loss: 19.210416657584055, Validation Loss: 20.974483489990234\n",
      "Epoch 593, Train Loss: 18.89815412248884, Validation Loss: 20.902677536010742\n",
      "Epoch 594, Train Loss: 18.792383807046072, Validation Loss: 20.758968830108643\n",
      "Epoch 595, Train Loss: 18.91606044769287, Validation Loss: 21.003553867340088\n",
      "Epoch 596, Train Loss: 19.043399333953857, Validation Loss: 21.054606914520264\n",
      "Epoch 597, Train Loss: 18.86993142536708, Validation Loss: 21.15193748474121\n",
      "Epoch 598, Train Loss: 18.884052072252548, Validation Loss: 21.063379287719727\n",
      "Epoch 599, Train Loss: 18.808088098253524, Validation Loss: 20.75680112838745\n",
      "Epoch 600, Train Loss: 18.827800955091202, Validation Loss: 20.792116165161133\n",
      "Epoch 601, Train Loss: 18.837809767041886, Validation Loss: 20.969782829284668\n",
      "Epoch 602, Train Loss: 19.42409474509103, Validation Loss: 22.738327980041504\n",
      "Epoch 603, Train Loss: 19.40970958982195, Validation Loss: 21.22544527053833\n",
      "Epoch 604, Train Loss: 18.946468625749862, Validation Loss: 20.904993534088135\n",
      "Epoch 605, Train Loss: 18.92617974962507, Validation Loss: 20.92444372177124\n",
      "Epoch 606, Train Loss: 19.014041696275985, Validation Loss: 20.977160930633545\n",
      "Epoch 607, Train Loss: 19.329622268676758, Validation Loss: 22.613823890686035\n",
      "Epoch 608, Train Loss: 19.351538521902903, Validation Loss: 20.896281242370605\n",
      "Epoch 609, Train Loss: 18.930739062173025, Validation Loss: 21.407447338104248\n",
      "Epoch 610, Train Loss: 19.01176963533674, Validation Loss: 21.101871013641357\n",
      "Epoch 611, Train Loss: 18.8393600327628, Validation Loss: 21.338566303253174\n",
      "Epoch 612, Train Loss: 18.858508723122732, Validation Loss: 20.85013484954834\n",
      "Epoch 613, Train Loss: 18.802397319248744, Validation Loss: 20.919959545135498\n",
      "Epoch 614, Train Loss: 18.749087197440012, Validation Loss: 20.750999927520752\n",
      "Epoch 615, Train Loss: 18.833977563040598, Validation Loss: 21.217852115631104\n",
      "Epoch 616, Train Loss: 18.914969171796525, Validation Loss: 21.09556484222412\n",
      "Epoch 617, Train Loss: 18.92727075304304, Validation Loss: 20.99064826965332\n",
      "Epoch 618, Train Loss: 19.082559653690883, Validation Loss: 21.18924856185913\n",
      "Epoch 619, Train Loss: 18.939831461225236, Validation Loss: 21.08286762237549\n",
      "Epoch 620, Train Loss: 19.005920478275844, Validation Loss: 20.79952621459961\n",
      "Epoch 621, Train Loss: 18.8281763621739, Validation Loss: 20.903316020965576\n",
      "Epoch 622, Train Loss: 18.826943397521973, Validation Loss: 20.830782413482666\n",
      "Epoch 623, Train Loss: 19.022546700068883, Validation Loss: 21.13896894454956\n",
      "Epoch 624, Train Loss: 18.971033096313477, Validation Loss: 20.933197021484375\n",
      "Epoch 625, Train Loss: 18.828843661717006, Validation Loss: 20.773412704467773\n",
      "Epoch 626, Train Loss: 18.809256076812744, Validation Loss: 20.96610164642334\n",
      "Epoch 627, Train Loss: 19.19863224029541, Validation Loss: 20.984477996826172\n",
      "Epoch 628, Train Loss: 18.97140802655901, Validation Loss: 20.960626125335693\n",
      "Epoch 629, Train Loss: 18.872330120631627, Validation Loss: 20.85101318359375\n",
      "Epoch 630, Train Loss: 18.870605877467565, Validation Loss: 21.27716875076294\n",
      "Epoch 631, Train Loss: 18.836688791002548, Validation Loss: 20.856051921844482\n",
      "Epoch 632, Train Loss: 18.79639298575265, Validation Loss: 20.841981887817383\n",
      "Epoch 633, Train Loss: 18.815631457737513, Validation Loss: 20.812342643737793\n",
      "Epoch 634, Train Loss: 18.78507219042097, Validation Loss: 20.8680157661438\n",
      "Epoch 635, Train Loss: 19.052237817219325, Validation Loss: 21.150757312774658\n",
      "Epoch 636, Train Loss: 18.86735119138445, Validation Loss: 20.808395862579346\n",
      "Epoch 637, Train Loss: 18.82331051145281, Validation Loss: 21.14024782180786\n",
      "Epoch 638, Train Loss: 18.835686479296005, Validation Loss: 20.86451768875122\n",
      "Epoch 639, Train Loss: 18.889695984976633, Validation Loss: 20.926709175109863\n",
      "Epoch 640, Train Loss: 18.90590967450823, Validation Loss: 20.825256824493408\n",
      "Epoch 641, Train Loss: 18.831135817936488, Validation Loss: 20.872614860534668\n",
      "Epoch 642, Train Loss: 19.021392345428467, Validation Loss: 21.394808769226074\n",
      "Epoch 643, Train Loss: 19.09218508856637, Validation Loss: 20.836956024169922\n",
      "Epoch 644, Train Loss: 18.8060302734375, Validation Loss: 20.79842185974121\n",
      "Epoch 645, Train Loss: 18.89836835861206, Validation Loss: 20.807506561279297\n",
      "Epoch 646, Train Loss: 19.012527057102748, Validation Loss: 21.076834201812744\n",
      "Epoch 647, Train Loss: 18.879142420632498, Validation Loss: 21.073164463043213\n",
      "Epoch 648, Train Loss: 18.949906281062535, Validation Loss: 20.8169264793396\n",
      "Epoch 649, Train Loss: 18.878212315695627, Validation Loss: 20.889959812164307\n",
      "Epoch 650, Train Loss: 18.817628587995255, Validation Loss: 21.37377643585205\n",
      "Epoch 651, Train Loss: 18.81995967456273, Validation Loss: 20.82958745956421\n",
      "Epoch 652, Train Loss: 18.833637986864364, Validation Loss: 20.870612144470215\n",
      "Epoch 653, Train Loss: 19.016142777034215, Validation Loss: 22.088852405548096\n",
      "Epoch 654, Train Loss: 19.31793975830078, Validation Loss: 21.183607578277588\n",
      "Epoch 655, Train Loss: 18.97009059361049, Validation Loss: 21.202245712280273\n",
      "Epoch 656, Train Loss: 18.99119738170079, Validation Loss: 21.550061225891113\n",
      "Epoch 657, Train Loss: 18.92049891608102, Validation Loss: 20.811121940612793\n",
      "Epoch 658, Train Loss: 18.94130836214338, Validation Loss: 21.138803005218506\n",
      "Epoch 659, Train Loss: 18.878835814339773, Validation Loss: 20.7342586517334\n",
      "Epoch 660, Train Loss: 18.803240094866073, Validation Loss: 20.755542278289795\n",
      "Epoch 661, Train Loss: 18.896201814923966, Validation Loss: 20.78907585144043\n",
      "Epoch 662, Train Loss: 18.84280470439366, Validation Loss: 20.787426948547363\n",
      "Epoch 663, Train Loss: 18.88022177559989, Validation Loss: 20.94755220413208\n",
      "Epoch 664, Train Loss: 18.933799539293563, Validation Loss: 20.76534605026245\n",
      "Epoch 665, Train Loss: 18.850109917776926, Validation Loss: 20.805950164794922\n",
      "Epoch 666, Train Loss: 18.87340416227068, Validation Loss: 21.071819305419922\n",
      "Epoch 667, Train Loss: 18.769123554229736, Validation Loss: 20.723348140716553\n",
      "Epoch 668, Train Loss: 19.039041996002197, Validation Loss: 20.97744083404541\n",
      "Epoch 669, Train Loss: 18.883783953530447, Validation Loss: 20.797728061676025\n",
      "Epoch 670, Train Loss: 18.818611553737096, Validation Loss: 20.710318565368652\n",
      "Epoch 671, Train Loss: 18.746321541922434, Validation Loss: 20.80567455291748\n",
      "Epoch 672, Train Loss: 18.91496685573033, Validation Loss: 21.014033317565918\n",
      "Epoch 673, Train Loss: 18.927415711539133, Validation Loss: 21.044312953948975\n",
      "Epoch 674, Train Loss: 18.92060273034232, Validation Loss: 21.140594959259033\n",
      "Epoch 675, Train Loss: 19.146690096173966, Validation Loss: 21.34928846359253\n",
      "Epoch 676, Train Loss: 18.85014751979283, Validation Loss: 20.742581844329834\n",
      "Epoch 677, Train Loss: 18.919052396501815, Validation Loss: 21.139130115509033\n",
      "Epoch 678, Train Loss: 18.95962177004133, Validation Loss: 20.816836833953857\n",
      "Epoch 679, Train Loss: 19.04027250834874, Validation Loss: 21.094441413879395\n",
      "Epoch 680, Train Loss: 18.97255059650966, Validation Loss: 20.83864688873291\n",
      "Epoch 681, Train Loss: 18.79031208583287, Validation Loss: 20.710585117340088\n",
      "Epoch 682, Train Loss: 19.084297282355173, Validation Loss: 21.177951335906982\n",
      "Epoch 683, Train Loss: 18.878791196005686, Validation Loss: 21.405620098114014\n",
      "Epoch 684, Train Loss: 18.954351084572927, Validation Loss: 20.793663024902344\n",
      "Epoch 685, Train Loss: 18.861334936959402, Validation Loss: 21.169643878936768\n",
      "Epoch 686, Train Loss: 18.926599093845912, Validation Loss: 20.82179594039917\n",
      "Epoch 687, Train Loss: 18.903423854282924, Validation Loss: 21.068645477294922\n",
      "Epoch 688, Train Loss: 18.89361081804548, Validation Loss: 20.765809059143066\n",
      "Epoch 689, Train Loss: 18.83402626855033, Validation Loss: 20.764105319976807\n",
      "Epoch 690, Train Loss: 19.061512027468, Validation Loss: 21.247018814086914\n",
      "Epoch 691, Train Loss: 18.997753756386892, Validation Loss: 20.906784057617188\n",
      "Epoch 692, Train Loss: 18.821737834385463, Validation Loss: 20.941711902618408\n",
      "Epoch 693, Train Loss: 18.731911999838694, Validation Loss: 20.80161714553833\n",
      "Epoch 694, Train Loss: 18.928924901144846, Validation Loss: 20.848662853240967\n",
      "Epoch 695, Train Loss: 18.890104940959386, Validation Loss: 20.754152297973633\n",
      "Epoch 696, Train Loss: 18.83270365851266, Validation Loss: 20.810741901397705\n",
      "Epoch 697, Train Loss: 18.859794276101248, Validation Loss: 20.90677547454834\n",
      "Epoch 698, Train Loss: 18.766600268227712, Validation Loss: 20.717028617858887\n",
      "Epoch 699, Train Loss: 18.79755108697074, Validation Loss: 20.88197946548462\n",
      "Epoch 700, Train Loss: 18.788925988333567, Validation Loss: 20.759978771209717\n",
      "Epoch 701, Train Loss: 18.756245408739364, Validation Loss: 20.786255836486816\n",
      "Epoch 702, Train Loss: 18.78661332811628, Validation Loss: 21.405872344970703\n",
      "Epoch 703, Train Loss: 19.07567276273455, Validation Loss: 21.11736536026001\n",
      "Epoch 704, Train Loss: 18.858200277600968, Validation Loss: 20.87872552871704\n",
      "Epoch 705, Train Loss: 18.74501916340419, Validation Loss: 21.213271617889404\n",
      "Epoch 706, Train Loss: 19.27023308617728, Validation Loss: 20.9192156791687\n",
      "Epoch 707, Train Loss: 18.79073531287057, Validation Loss: 20.826777458190918\n",
      "Epoch 708, Train Loss: 18.875178950173513, Validation Loss: 20.84588861465454\n",
      "Epoch 709, Train Loss: 18.810849530356272, Validation Loss: 20.89214324951172\n",
      "Epoch 710, Train Loss: 18.82290506362915, Validation Loss: 20.728801727294922\n",
      "Epoch 711, Train Loss: 18.784909180232457, Validation Loss: 20.766765117645264\n",
      "Epoch 712, Train Loss: 18.863211972372874, Validation Loss: 21.40583562850952\n",
      "Epoch 713, Train Loss: 19.191232170377457, Validation Loss: 21.3930082321167\n",
      "Epoch 714, Train Loss: 19.069197382245743, Validation Loss: 20.983848571777344\n",
      "Epoch 715, Train Loss: 18.969883850642614, Validation Loss: 21.119884967803955\n",
      "Epoch 716, Train Loss: 18.851846831185476, Validation Loss: 20.92475986480713\n",
      "Epoch 717, Train Loss: 18.805082525525773, Validation Loss: 21.084789752960205\n",
      "Epoch 718, Train Loss: 18.841355800628662, Validation Loss: 21.081757068634033\n",
      "Epoch 719, Train Loss: 18.942341259547643, Validation Loss: 20.851315021514893\n",
      "Epoch 720, Train Loss: 18.91407081059047, Validation Loss: 20.91009283065796\n",
      "Epoch 721, Train Loss: 18.900595120021276, Validation Loss: 20.80704927444458\n",
      "Epoch 722, Train Loss: 18.74336556025914, Validation Loss: 20.805434703826904\n",
      "Epoch 723, Train Loss: 18.841741834368026, Validation Loss: 20.819411754608154\n",
      "Epoch 724, Train Loss: 18.708879470825195, Validation Loss: 20.871846675872803\n",
      "Epoch 725, Train Loss: 18.81467342376709, Validation Loss: 20.70998191833496\n",
      "Epoch 726, Train Loss: 18.73311642238072, Validation Loss: 20.87356662750244\n",
      "Epoch 727, Train Loss: 18.83969361441476, Validation Loss: 20.73849058151245\n",
      "Epoch 728, Train Loss: 18.696274621146067, Validation Loss: 20.670653343200684\n",
      "Epoch 729, Train Loss: 19.064863273075648, Validation Loss: 21.12581491470337\n",
      "Epoch 730, Train Loss: 18.875658103397914, Validation Loss: 20.800148487091064\n",
      "Epoch 731, Train Loss: 18.784314019339426, Validation Loss: 20.71744203567505\n",
      "Epoch 732, Train Loss: 18.747089794703893, Validation Loss: 20.737995147705078\n",
      "Epoch 733, Train Loss: 18.917436122894287, Validation Loss: 21.12129497528076\n",
      "Epoch 734, Train Loss: 19.02558762686593, Validation Loss: 20.964102268218994\n",
      "Epoch 735, Train Loss: 19.27512284687587, Validation Loss: 21.98476219177246\n",
      "Epoch 736, Train Loss: 19.13864823750087, Validation Loss: 20.932615280151367\n",
      "Epoch 737, Train Loss: 19.03244243349348, Validation Loss: 20.863874435424805\n",
      "Epoch 738, Train Loss: 18.813440118517196, Validation Loss: 21.110718727111816\n",
      "Epoch 739, Train Loss: 18.77694593157087, Validation Loss: 20.764659881591797\n",
      "Epoch 740, Train Loss: 18.75296756199428, Validation Loss: 20.6723895072937\n",
      "Epoch 741, Train Loss: 18.766444751194545, Validation Loss: 20.684996604919434\n",
      "Epoch 742, Train Loss: 18.80408784321376, Validation Loss: 21.818240642547607\n",
      "Epoch 743, Train Loss: 19.05833625793457, Validation Loss: 20.90632438659668\n",
      "Epoch 744, Train Loss: 18.903812135968888, Validation Loss: 21.16507911682129\n",
      "Epoch 745, Train Loss: 18.882475989205496, Validation Loss: 20.733344554901123\n",
      "Epoch 746, Train Loss: 18.889409678322927, Validation Loss: 20.886204719543457\n",
      "Epoch 747, Train Loss: 18.755509785243444, Validation Loss: 20.70400094985962\n",
      "Epoch 748, Train Loss: 18.74846397127424, Validation Loss: 21.114498138427734\n",
      "Epoch 749, Train Loss: 19.209937844957626, Validation Loss: 20.961897373199463\n",
      "Epoch 750, Train Loss: 18.951561519077845, Validation Loss: 20.78605079650879\n",
      "Epoch 751, Train Loss: 18.87732240131923, Validation Loss: 21.066160202026367\n",
      "Epoch 752, Train Loss: 19.11094924381801, Validation Loss: 20.871386528015137\n",
      "Epoch 753, Train Loss: 18.815339020320348, Validation Loss: 20.756630897521973\n",
      "Epoch 754, Train Loss: 18.81280299595424, Validation Loss: 21.168899536132812\n",
      "Epoch 755, Train Loss: 18.84225606918335, Validation Loss: 20.794873237609863\n",
      "Epoch 756, Train Loss: 18.757286276136124, Validation Loss: 20.914527416229248\n",
      "Epoch 757, Train Loss: 18.907019206455775, Validation Loss: 20.7677321434021\n",
      "Epoch 758, Train Loss: 19.404685633523123, Validation Loss: 21.618704319000244\n",
      "Epoch 759, Train Loss: 19.04857928412301, Validation Loss: 20.96010684967041\n",
      "Epoch 760, Train Loss: 18.964922973087855, Validation Loss: 20.824951171875\n",
      "Epoch 761, Train Loss: 18.928336484091624, Validation Loss: 20.85511350631714\n",
      "Epoch 762, Train Loss: 18.870659010750906, Validation Loss: 20.701624870300293\n",
      "Epoch 763, Train Loss: 18.816113131386892, Validation Loss: 20.757112979888916\n",
      "Epoch 764, Train Loss: 18.8620742389134, Validation Loss: 20.79591417312622\n",
      "Epoch 765, Train Loss: 18.873414039611816, Validation Loss: 20.87313938140869\n",
      "Epoch 766, Train Loss: 18.872334003448486, Validation Loss: 20.805498600006104\n",
      "Epoch 767, Train Loss: 18.749950340815953, Validation Loss: 20.688021183013916\n",
      "Epoch 768, Train Loss: 18.840089116777694, Validation Loss: 21.1328067779541\n",
      "Epoch 769, Train Loss: 19.026615824018204, Validation Loss: 21.4105486869812\n",
      "Epoch 770, Train Loss: 19.50334964479719, Validation Loss: 21.522392749786377\n",
      "Epoch 771, Train Loss: 19.111109801701136, Validation Loss: 20.883384704589844\n",
      "Epoch 772, Train Loss: 18.83682918548584, Validation Loss: 20.9951434135437\n",
      "Epoch 773, Train Loss: 18.79192658833095, Validation Loss: 20.66975736618042\n",
      "Epoch 774, Train Loss: 18.97911766597203, Validation Loss: 21.05791711807251\n",
      "Epoch 775, Train Loss: 19.10285758972168, Validation Loss: 20.801522254943848\n",
      "Epoch 776, Train Loss: 18.78356667927333, Validation Loss: 20.74924659729004\n",
      "Epoch 777, Train Loss: 18.79585463660104, Validation Loss: 20.685975074768066\n",
      "Epoch 778, Train Loss: 19.064007895333425, Validation Loss: 21.639053344726562\n",
      "Epoch 779, Train Loss: 18.87919773374285, Validation Loss: 20.735517501831055\n",
      "Epoch 780, Train Loss: 18.713977200644358, Validation Loss: 20.750588417053223\n",
      "Epoch 781, Train Loss: 18.74938304083688, Validation Loss: 20.747949600219727\n",
      "Epoch 782, Train Loss: 18.77122940335955, Validation Loss: 20.748835563659668\n",
      "Epoch 783, Train Loss: 18.84104517527989, Validation Loss: 20.777013301849365\n",
      "Epoch 784, Train Loss: 18.865638528551376, Validation Loss: 20.89368772506714\n",
      "Epoch 785, Train Loss: 18.88764456340245, Validation Loss: 20.841036796569824\n",
      "Epoch 786, Train Loss: 18.77551371710641, Validation Loss: 20.72774362564087\n",
      "Epoch 787, Train Loss: 18.718028000422887, Validation Loss: 20.818615436553955\n",
      "Epoch 788, Train Loss: 18.788904530661448, Validation Loss: 21.015276432037354\n",
      "Epoch 789, Train Loss: 18.868789059775217, Validation Loss: 20.99761438369751\n",
      "Epoch 790, Train Loss: 18.961138929639542, Validation Loss: 20.725252151489258\n",
      "Epoch 791, Train Loss: 18.94022410256522, Validation Loss: 20.965521335601807\n",
      "Epoch 792, Train Loss: 18.752647059304373, Validation Loss: 20.90937852859497\n",
      "Epoch 793, Train Loss: 18.915309361049108, Validation Loss: 20.954909801483154\n",
      "Epoch 794, Train Loss: 18.8096707207816, Validation Loss: 20.729060173034668\n",
      "Epoch 795, Train Loss: 18.75530638013567, Validation Loss: 20.678697109222412\n",
      "Epoch 796, Train Loss: 18.67782654081072, Validation Loss: 20.70147466659546\n",
      "Epoch 797, Train Loss: 18.779448441096715, Validation Loss: 20.783451080322266\n",
      "Epoch 798, Train Loss: 18.71506670543126, Validation Loss: 20.824520587921143\n",
      "Epoch 799, Train Loss: 19.10091577257429, Validation Loss: 21.121131896972656\n",
      "Epoch 800, Train Loss: 18.9717835017613, Validation Loss: 20.817012786865234\n",
      "Epoch 801, Train Loss: 18.798591000693186, Validation Loss: 20.756157398223877\n",
      "Epoch 802, Train Loss: 18.79159014565604, Validation Loss: 20.84123945236206\n",
      "Epoch 803, Train Loss: 18.7452210017613, Validation Loss: 20.760478973388672\n",
      "Epoch 804, Train Loss: 18.77544927597046, Validation Loss: 20.727187156677246\n",
      "Epoch 805, Train Loss: 19.015899521963938, Validation Loss: 21.107279777526855\n",
      "Epoch 806, Train Loss: 19.1587872505188, Validation Loss: 20.97404146194458\n",
      "Epoch 807, Train Loss: 18.871881689344132, Validation Loss: 20.773069381713867\n",
      "Epoch 808, Train Loss: 18.64051832471575, Validation Loss: 20.946377754211426\n",
      "Epoch 809, Train Loss: 18.901502064296178, Validation Loss: 20.697304248809814\n",
      "Epoch 810, Train Loss: 18.7126179422651, Validation Loss: 20.784828662872314\n",
      "Epoch 811, Train Loss: 18.742987905229842, Validation Loss: 20.648754119873047\n",
      "Epoch 812, Train Loss: 18.738746574946813, Validation Loss: 20.721686840057373\n",
      "Epoch 813, Train Loss: 18.837427071162633, Validation Loss: 21.374996662139893\n",
      "Epoch 814, Train Loss: 18.96456582205636, Validation Loss: 21.061919689178467\n",
      "Epoch 815, Train Loss: 19.087030138288224, Validation Loss: 20.94291400909424\n",
      "Epoch 816, Train Loss: 18.72053119114467, Validation Loss: 20.848615646362305\n",
      "Epoch 817, Train Loss: 18.72562871660505, Validation Loss: 20.727023124694824\n",
      "Epoch 818, Train Loss: 18.83207811628069, Validation Loss: 20.75962257385254\n",
      "Epoch 819, Train Loss: 18.781824043818883, Validation Loss: 20.747519493103027\n",
      "Epoch 820, Train Loss: 19.129496506282262, Validation Loss: 21.095885753631592\n",
      "Epoch 821, Train Loss: 18.888279574257986, Validation Loss: 20.686789989471436\n",
      "Epoch 822, Train Loss: 18.798845972333634, Validation Loss: 20.79289150238037\n",
      "Epoch 823, Train Loss: 18.777779851640975, Validation Loss: 21.17179584503174\n",
      "Epoch 824, Train Loss: 18.86746883392334, Validation Loss: 20.684363842010498\n",
      "Epoch 825, Train Loss: 18.701935631888254, Validation Loss: 20.724430561065674\n",
      "Epoch 826, Train Loss: 18.74561677660261, Validation Loss: 20.665529251098633\n",
      "Epoch 827, Train Loss: 18.72749192374093, Validation Loss: 20.78653335571289\n",
      "Epoch 828, Train Loss: 18.77841922215053, Validation Loss: 21.65729856491089\n",
      "Epoch 829, Train Loss: 19.02420221056257, Validation Loss: 20.807825565338135\n",
      "Epoch 830, Train Loss: 18.821397032056534, Validation Loss: 21.07938241958618\n",
      "Epoch 831, Train Loss: 19.550310679844447, Validation Loss: 21.48683214187622\n",
      "Epoch 832, Train Loss: 19.098861081259592, Validation Loss: 20.982565879821777\n",
      "Epoch 833, Train Loss: 18.810688427516393, Validation Loss: 20.934409141540527\n",
      "Epoch 834, Train Loss: 18.78103576387678, Validation Loss: 20.833147048950195\n",
      "Epoch 835, Train Loss: 18.842157976967947, Validation Loss: 21.391615867614746\n",
      "Epoch 836, Train Loss: 18.852629320962087, Validation Loss: 20.81096315383911\n",
      "Epoch 837, Train Loss: 18.72473921094622, Validation Loss: 20.67311429977417\n",
      "Epoch 838, Train Loss: 18.721322911126272, Validation Loss: 20.665306091308594\n",
      "Epoch 839, Train Loss: 18.97399296079363, Validation Loss: 20.867873191833496\n",
      "Epoch 840, Train Loss: 19.005987235477992, Validation Loss: 20.909445762634277\n",
      "Epoch 841, Train Loss: 18.900863579341344, Validation Loss: 20.80760097503662\n",
      "Epoch 842, Train Loss: 18.749235766274587, Validation Loss: 20.710845947265625\n",
      "Epoch 843, Train Loss: 18.900676727294922, Validation Loss: 20.729703903198242\n",
      "Epoch 844, Train Loss: 18.740244592939103, Validation Loss: 20.70094108581543\n",
      "Epoch 845, Train Loss: 18.839574813842773, Validation Loss: 20.751835823059082\n",
      "Epoch 846, Train Loss: 18.792357512882777, Validation Loss: 20.872692108154297\n",
      "Epoch 847, Train Loss: 18.73738636289324, Validation Loss: 20.682088375091553\n",
      "Epoch 848, Train Loss: 18.737647942134313, Validation Loss: 21.175171375274658\n",
      "Epoch 849, Train Loss: 19.154348645891464, Validation Loss: 22.58014154434204\n",
      "Epoch 850, Train Loss: 19.180222988128662, Validation Loss: 20.838207244873047\n",
      "Epoch 851, Train Loss: 18.76335266658238, Validation Loss: 20.706143856048584\n",
      "Epoch 852, Train Loss: 18.72441475731986, Validation Loss: 20.814210414886475\n",
      "Epoch 853, Train Loss: 18.791488919939315, Validation Loss: 20.671401977539062\n",
      "Epoch 854, Train Loss: 18.705451216016495, Validation Loss: 20.674480438232422\n",
      "Epoch 855, Train Loss: 18.754486628941127, Validation Loss: 20.631954193115234\n",
      "Epoch 856, Train Loss: 18.884934765951975, Validation Loss: 20.775988578796387\n",
      "Epoch 857, Train Loss: 18.944163390568324, Validation Loss: 20.807434558868408\n",
      "Epoch 858, Train Loss: 18.785076481955393, Validation Loss: 20.804609775543213\n",
      "Epoch 859, Train Loss: 18.71777084895543, Validation Loss: 20.682637214660645\n",
      "Epoch 860, Train Loss: 18.950055939810618, Validation Loss: 20.787381172180176\n",
      "Epoch 861, Train Loss: 18.709135055541992, Validation Loss: 21.252936840057373\n",
      "Epoch 862, Train Loss: 18.999408313206263, Validation Loss: 20.7963809967041\n",
      "Epoch 863, Train Loss: 18.80049099240984, Validation Loss: 20.831029415130615\n",
      "Epoch 864, Train Loss: 18.773548671177455, Validation Loss: 20.923688411712646\n",
      "Epoch 865, Train Loss: 19.00414882387434, Validation Loss: 20.70607042312622\n",
      "Epoch 866, Train Loss: 18.781691551208496, Validation Loss: 20.976701259613037\n",
      "Epoch 867, Train Loss: 19.107935701097762, Validation Loss: 21.045170783996582\n",
      "Epoch 868, Train Loss: 18.794109412602015, Validation Loss: 20.726537704467773\n",
      "Epoch 869, Train Loss: 19.19728524344308, Validation Loss: 21.06759262084961\n",
      "Epoch 870, Train Loss: 18.90878016608102, Validation Loss: 20.812135696411133\n",
      "Epoch 871, Train Loss: 19.18689727783203, Validation Loss: 21.039056301116943\n",
      "Epoch 872, Train Loss: 18.88485608782087, Validation Loss: 20.797616004943848\n",
      "Epoch 873, Train Loss: 18.766101700919016, Validation Loss: 20.83210849761963\n",
      "Epoch 874, Train Loss: 18.763912677764893, Validation Loss: 20.664301872253418\n",
      "Epoch 875, Train Loss: 18.992577620915004, Validation Loss: 20.781885623931885\n",
      "Epoch 876, Train Loss: 18.80082450594221, Validation Loss: 20.707520961761475\n",
      "Epoch 877, Train Loss: 18.787070683070592, Validation Loss: 20.707115650177002\n",
      "Epoch 878, Train Loss: 18.77361903871809, Validation Loss: 20.65055227279663\n",
      "Epoch 879, Train Loss: 18.758577619280135, Validation Loss: 20.654451370239258\n",
      "Epoch 880, Train Loss: 18.800905023302352, Validation Loss: 20.67668628692627\n",
      "Epoch 881, Train Loss: 18.9019455909729, Validation Loss: 20.754257202148438\n",
      "Epoch 882, Train Loss: 18.74709565298898, Validation Loss: 20.716328620910645\n",
      "Epoch 883, Train Loss: 18.70399420601981, Validation Loss: 20.627156257629395\n",
      "Epoch 884, Train Loss: 18.752407210213796, Validation Loss: 20.682621002197266\n",
      "Epoch 885, Train Loss: 18.77700969151088, Validation Loss: 20.975292682647705\n",
      "Epoch 886, Train Loss: 18.78906992503575, Validation Loss: 20.832176685333252\n",
      "Epoch 887, Train Loss: 18.738609790802002, Validation Loss: 21.082368850708008\n",
      "Epoch 888, Train Loss: 18.80336597987584, Validation Loss: 20.67973041534424\n",
      "Epoch 889, Train Loss: 18.84677335194179, Validation Loss: 21.004787921905518\n",
      "Epoch 890, Train Loss: 18.771646635872976, Validation Loss: 20.745359420776367\n",
      "Epoch 891, Train Loss: 18.848829916545323, Validation Loss: 21.476447105407715\n",
      "Epoch 892, Train Loss: 18.923892974853516, Validation Loss: 20.801146507263184\n",
      "Epoch 893, Train Loss: 18.769157205309188, Validation Loss: 20.6662917137146\n",
      "Epoch 894, Train Loss: 18.674328804016113, Validation Loss: 20.664633750915527\n",
      "Epoch 895, Train Loss: 18.878542287009104, Validation Loss: 20.958600997924805\n",
      "Epoch 896, Train Loss: 18.738202026912145, Validation Loss: 20.862181186676025\n",
      "Epoch 897, Train Loss: 18.69438430241176, Validation Loss: 20.71156358718872\n",
      "Epoch 898, Train Loss: 18.696423734937394, Validation Loss: 20.62726926803589\n",
      "Epoch 899, Train Loss: 18.838374410356796, Validation Loss: 20.910876750946045\n",
      "Epoch 900, Train Loss: 18.86224106379918, Validation Loss: 20.695342540740967\n",
      "Epoch 901, Train Loss: 18.730207647596085, Validation Loss: 20.653067588806152\n",
      "Epoch 902, Train Loss: 18.721227850232804, Validation Loss: 21.358377933502197\n",
      "Epoch 903, Train Loss: 18.8316707611084, Validation Loss: 21.196635246276855\n",
      "Epoch 904, Train Loss: 18.754968302590505, Validation Loss: 20.715797901153564\n",
      "Epoch 905, Train Loss: 18.7332409790584, Validation Loss: 20.838457107543945\n",
      "Epoch 906, Train Loss: 18.863647188459122, Validation Loss: 20.745727062225342\n",
      "Epoch 907, Train Loss: 18.78576639720372, Validation Loss: 20.79944896697998\n",
      "Epoch 908, Train Loss: 18.8214909689767, Validation Loss: 20.945552825927734\n",
      "Epoch 909, Train Loss: 18.76455041340419, Validation Loss: 20.83268928527832\n",
      "Epoch 910, Train Loss: 18.85407406943185, Validation Loss: 20.778849124908447\n",
      "Epoch 911, Train Loss: 18.785192693982804, Validation Loss: 20.816919326782227\n",
      "Epoch 912, Train Loss: 18.824479239327566, Validation Loss: 20.728941917419434\n",
      "Epoch 913, Train Loss: 18.70534590312413, Validation Loss: 20.69192123413086\n",
      "Epoch 914, Train Loss: 18.67992149080549, Validation Loss: 20.929051399230957\n",
      "Epoch 915, Train Loss: 18.716847487858363, Validation Loss: 20.902591228485107\n",
      "Epoch 916, Train Loss: 18.951081820896693, Validation Loss: 20.84124755859375\n",
      "Epoch 917, Train Loss: 18.890194416046143, Validation Loss: 20.900843620300293\n",
      "Epoch 918, Train Loss: 18.94780022757394, Validation Loss: 20.767533779144287\n",
      "Epoch 919, Train Loss: 18.856999397277832, Validation Loss: 20.70713233947754\n",
      "Epoch 920, Train Loss: 18.728725569588796, Validation Loss: 20.698360443115234\n",
      "Epoch 921, Train Loss: 18.663142885480607, Validation Loss: 20.649620056152344\n",
      "Epoch 922, Train Loss: 18.736607483455114, Validation Loss: 20.781687259674072\n",
      "Epoch 923, Train Loss: 18.6981657573155, Validation Loss: 20.63356590270996\n",
      "Epoch 924, Train Loss: 18.692600999559676, Validation Loss: 20.73212480545044\n",
      "Epoch 925, Train Loss: 18.875356946672714, Validation Loss: 20.793948650360107\n",
      "Epoch 926, Train Loss: 18.762399877820695, Validation Loss: 20.767226696014404\n",
      "Epoch 927, Train Loss: 18.782585280282156, Validation Loss: 20.656569004058838\n",
      "Epoch 928, Train Loss: 19.031678199768066, Validation Loss: 20.86443567276001\n",
      "Epoch 929, Train Loss: 18.83436073575701, Validation Loss: 20.677443027496338\n",
      "Epoch 930, Train Loss: 18.74338177272252, Validation Loss: 20.69148063659668\n",
      "Epoch 931, Train Loss: 18.693427971431188, Validation Loss: 21.046356201171875\n",
      "Epoch 932, Train Loss: 18.75260857173375, Validation Loss: 20.8027925491333\n",
      "Epoch 933, Train Loss: 18.771538734436035, Validation Loss: 20.727649211883545\n",
      "Epoch 934, Train Loss: 18.674132892063685, Validation Loss: 20.967233657836914\n",
      "Epoch 935, Train Loss: 18.722969940730504, Validation Loss: 20.751850128173828\n",
      "Epoch 936, Train Loss: 18.676791054861887, Validation Loss: 20.67886209487915\n",
      "Epoch 937, Train Loss: 18.708836351122176, Validation Loss: 20.62582302093506\n",
      "Epoch 938, Train Loss: 18.763076032911027, Validation Loss: 20.660069942474365\n",
      "Epoch 939, Train Loss: 18.736677850995743, Validation Loss: 20.66007661819458\n",
      "Epoch 940, Train Loss: 18.686101164136613, Validation Loss: 20.68769073486328\n",
      "Epoch 941, Train Loss: 18.991440500531876, Validation Loss: 22.017858028411865\n",
      "Epoch 942, Train Loss: 18.979185036250524, Validation Loss: 21.0107364654541\n",
      "Epoch 943, Train Loss: 18.72055264881679, Validation Loss: 20.646246910095215\n",
      "Epoch 944, Train Loss: 18.719151156289236, Validation Loss: 20.648355960845947\n",
      "Epoch 945, Train Loss: 18.75498948778425, Validation Loss: 20.806528091430664\n",
      "Epoch 946, Train Loss: 18.852697440556117, Validation Loss: 20.96145486831665\n",
      "Epoch 947, Train Loss: 18.97717775617327, Validation Loss: 20.983009815216064\n",
      "Epoch 948, Train Loss: 18.714380468641007, Validation Loss: 20.641581535339355\n",
      "Epoch 949, Train Loss: 18.798102583203995, Validation Loss: 21.052293300628662\n",
      "Epoch 950, Train Loss: 18.797046457018173, Validation Loss: 20.745003700256348\n",
      "Epoch 951, Train Loss: 18.7424270766122, Validation Loss: 20.66376781463623\n",
      "Epoch 952, Train Loss: 18.946115153176443, Validation Loss: 21.105141639709473\n",
      "Epoch 953, Train Loss: 18.843502589634486, Validation Loss: 20.869683265686035\n",
      "Epoch 954, Train Loss: 18.885888167790004, Validation Loss: 20.815298557281494\n",
      "Epoch 955, Train Loss: 18.75891855784825, Validation Loss: 20.924209594726562\n",
      "Epoch 956, Train Loss: 18.91696207863944, Validation Loss: 21.409000396728516\n",
      "Epoch 957, Train Loss: 18.942528792790004, Validation Loss: 20.762704849243164\n",
      "Epoch 958, Train Loss: 18.72237307684762, Validation Loss: 20.62110424041748\n",
      "Epoch 959, Train Loss: 18.838266917637416, Validation Loss: 20.630061626434326\n",
      "Epoch 960, Train Loss: 18.690852097102574, Validation Loss: 20.59935188293457\n",
      "Epoch 961, Train Loss: 18.79333131653922, Validation Loss: 20.66866636276245\n",
      "Epoch 962, Train Loss: 18.846604551587784, Validation Loss: 21.486565113067627\n",
      "Epoch 963, Train Loss: 18.884747164590017, Validation Loss: 20.990713596343994\n",
      "Epoch 964, Train Loss: 18.780020918164933, Validation Loss: 20.63173007965088\n",
      "Epoch 965, Train Loss: 18.801506110600062, Validation Loss: 21.151584148406982\n",
      "Epoch 966, Train Loss: 18.938660144805908, Validation Loss: 20.714714527130127\n",
      "Epoch 967, Train Loss: 18.80649083001273, Validation Loss: 20.85627555847168\n",
      "Epoch 968, Train Loss: 18.763425009591238, Validation Loss: 20.78905963897705\n",
      "Epoch 969, Train Loss: 18.688602549689158, Validation Loss: 20.585917949676514\n",
      "Epoch 970, Train Loss: 18.645400251661027, Validation Loss: 20.63502025604248\n",
      "Epoch 971, Train Loss: 18.738961628505162, Validation Loss: 20.75913906097412\n",
      "Epoch 972, Train Loss: 18.75522783824376, Validation Loss: 20.902151107788086\n",
      "Epoch 973, Train Loss: 18.7568256514413, Validation Loss: 20.690486907958984\n",
      "Epoch 974, Train Loss: 18.999309880392893, Validation Loss: 20.890034198760986\n",
      "Epoch 975, Train Loss: 18.85155473436628, Validation Loss: 20.796600818634033\n",
      "Epoch 976, Train Loss: 18.79055486406599, Validation Loss: 20.755449295043945\n",
      "Epoch 977, Train Loss: 18.802426474434988, Validation Loss: 21.005839824676514\n",
      "Epoch 978, Train Loss: 18.78036376408168, Validation Loss: 20.70119857788086\n",
      "Epoch 979, Train Loss: 18.70208556311471, Validation Loss: 20.69730043411255\n",
      "Epoch 980, Train Loss: 18.763950960976736, Validation Loss: 21.618596076965332\n",
      "Epoch 981, Train Loss: 18.777880600520543, Validation Loss: 21.123236656188965\n",
      "Epoch 982, Train Loss: 19.04963759013585, Validation Loss: 21.300751209259033\n",
      "Epoch 983, Train Loss: 18.886935302189418, Validation Loss: 20.76503086090088\n",
      "Epoch 984, Train Loss: 18.91076578412737, Validation Loss: 20.837564945220947\n",
      "Epoch 985, Train Loss: 18.851821490696498, Validation Loss: 20.686739921569824\n",
      "Epoch 986, Train Loss: 18.699747834886825, Validation Loss: 20.643909454345703\n",
      "Epoch 987, Train Loss: 18.731838839394705, Validation Loss: 20.7359356880188\n",
      "Epoch 988, Train Loss: 18.84187344142369, Validation Loss: 20.66079044342041\n",
      "Epoch 989, Train Loss: 18.712355409349716, Validation Loss: 20.678795337677002\n",
      "Epoch 990, Train Loss: 18.742233957563126, Validation Loss: 20.687185287475586\n",
      "Epoch 991, Train Loss: 18.656808716910227, Validation Loss: 20.705589294433594\n",
      "Epoch 992, Train Loss: 18.727682999202184, Validation Loss: 20.868412971496582\n",
      "Epoch 993, Train Loss: 18.855906452451432, Validation Loss: 20.759829998016357\n",
      "Epoch 994, Train Loss: 18.78396143232073, Validation Loss: 21.186992168426514\n",
      "Epoch 995, Train Loss: 18.73642178944179, Validation Loss: 20.7212872505188\n",
      "Epoch 996, Train Loss: 18.66107620511736, Validation Loss: 20.61457872390747\n",
      "Epoch 997, Train Loss: 18.712647369929723, Validation Loss: 20.63235378265381\n",
      "Epoch 998, Train Loss: 18.705555779593332, Validation Loss: 20.630857467651367\n",
      "Epoch 999, Train Loss: 18.926456655774796, Validation Loss: 21.201488494873047\n",
      "Epoch 1000, Train Loss: 19.19018370764596, Validation Loss: 21.96248197555542\n",
      "---- RESULTS Foster 50 ----\n",
      "DeltaE mean: 15.86\n",
      "DeltaE max: 52.59\n",
      "DeltaE median: 15.13\n",
      "DeltaE 95 percentile: 28.29\n",
      "DeltaE 99 percentile: 35.68\n"
     ]
    }
   ],
   "source": [
    "from models import RGBtoXYZNetwork\n",
    "from evaluate import pred\n",
    "mcdonalds_nn = RGBtoXYZNetwork()\n",
    "mcdonalds_nn.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(mcdonalds_nn, response_testset_camera, response_testset_xyz, \"Foster 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression(fit_intercept=False)\n",
    "\n",
    "print(np.max(response_trainset_camera))\n",
    "\n",
    "\n",
    "linear.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(linear, response_testset_camera, response_testset_xyz, \"Foster 50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from models import DeltaEOptimizer\n",
    "\n",
    "DE2000RP = Pipeline([\n",
    "    ('regressor', DeltaEOptimizer(root_polynomial=False, degree=1))\n",
    "])\n",
    "\n",
    "\n",
    "DE2000RP.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(DE2000RP, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit 3rd order Root-Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolynomialTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "RP_linear_3 = Pipeline([\n",
    "    ('transformer', PolynomialTransformer(degree=3, rp=True)),\n",
    "    ('regressor', LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "RP_linear_3.fit(response_trainset_camera, response_trainset_xyz)\n",
    "\n",
    "pred(RP_linear_3, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a 2nd order Root-Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GAMOptimizer, PolynomialTransformer, DeltaEOptimizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "RP_linear_2 = Pipeline([\n",
    "    ('transformer', PolynomialTransformer(degree=2, rp=True)),\n",
    "    ('regressor', LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "\n",
    "RP_linear_2.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(RP_linear_2, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Linear_3 = Pipeline([\n",
    "    ('transformer', PolynomialTransformer(degree=3, rp=False)),\n",
    "    ('regressor', LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "\n",
    "P_Linear_3.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(P_Linear_3, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_linear = Pipeline([\n",
    "    ('transformer', PolynomialTransformer(degree=2, rp=False)),\n",
    "    ('regressor', LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "\n",
    "P_linear.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(P_linear, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from models import DeltaEOptimizer\n",
    "\n",
    "DE2000P = Pipeline([\n",
    "    ('regressor', DeltaEOptimizer(root_polynomial=False, degree=3))\n",
    "])\n",
    "\n",
    "\n",
    "DE2000P.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(DE2000P, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from models import DeltaEOptimizer\n",
    "\n",
    "DE2000RP = Pipeline([\n",
    "    ('regressor', DeltaEOptimizer(root_polynomial=True, degree=3))\n",
    "])\n",
    "\n",
    "\n",
    "DE2000RP.fit(response_trainset_camera, response_trainset_xyz)\n",
    "pred(DE2000RP, response_testset_camera, response_testset_xyz, \"DeltaE Foster+CAVE\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf57dc982f5f7f7659d5105c425ec4d8a9bd6722199c7740e28d03845b9098c4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('dippa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
